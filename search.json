[
  {
    "objectID": "intro/versioning.html",
    "href": "intro/versioning.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "&lt; empty &gt;",
    "crumbs": [
      "Introduction",
      "Versioning and Release Notes"
    ]
  },
  {
    "objectID": "intro/how_started.html",
    "href": "intro/how_started.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "Accurate estimates on cropped area, crop type distributions and irrigation practices play an important role in guiding policy decisions related to food & water security. In 2023, the WorldCereal consortium released the first set of high-resolution, global and season-specific cropland, crop type and irrigation maps for the year 2021, thereby demonstrating both the scientific and technical feasibility of such a challenging undertaking.\n\n\n\nWorldCereal 2021 temporary crop extent map, resampled to ~0.004 ° resolution. Each pixel contains a number between 0 and 100, indicating the fractional cover of temporary crops within that pixel.\nBased on the success of this first phase of the project, the consortium now continues to open up the processing system to the world, powered by the Copernicus Data Space Ecosystem and OpenEO.\nDiscover more about the specific objectives and activities of this second phase of the project.",
    "crumbs": [
      "Introduction",
      "How it all started"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESA WorldCereal documentation portal",
    "section": "",
    "text": "ESA WorldCereal documentation portal\n\n  \n\nThe WorldCereal project aims to offer a processing system allowing anyone to train and apply custom cropland and/or crop type detection models using open Earth Observation (EO) data.\nThis documentation portal aims to provide:\n\nRelevant background information about the design of the system and implemented workflows\nPractical and step-by-step guidelines on how to use the processing system\n\nIn the INTRODUCTION section, we provide insights into the origin and scope of the system, as well as an overview of the functionalities of the different system components. The other sections are each devoted to a different system component, containing both conceptual and practical information required to get you started.\nAny remaining questions? Reach out to us through our dedicated USER FORUM!\nThe WorldCereal project is executed by a consortium of six partners, lead by VITO Remote Sensing:\n\nand funded by the European Space Agency (ESA)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "vdm/overview.html",
    "href": "vdm/overview.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "&lt; empty &gt;",
    "crumbs": [
      "Visualization and Dissemination Module (VDM)",
      "Overview"
    ]
  },
  {
    "objectID": "vdm/launch.html",
    "href": "vdm/launch.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "The Visualization and Disseminaton Module will host specific user interfaces allowing our users to launch their processing jobs with just a few clicks.\nThis feature is currently still under development…",
    "crumbs": [
      "Visualization and Dissemination Module (VDM)",
      "Launch Processing Jobs"
    ]
  },
  {
    "objectID": "rdm/upload.html",
    "href": "rdm/upload.html",
    "title": "Introduction",
    "section": "",
    "text": "////////// Jeroen comments ///////////////\nIntroduction\nGuidelines on data collection\n(important features of the data + best practices)\nUploading through User Interface (you need terrascope account!)\n–&gt; Refer to demo video!\nUsing your data in the processing module\n- Refer to previous section on retrieving private samples\n- (Refer to next section on input data extractions)\n////////// Jeroen comments ///////////////",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Upload Your Data"
    ]
  },
  {
    "objectID": "rdm/upload.html#uploading-through-user-interface-you-need-terrascope-account",
    "href": "rdm/upload.html#uploading-through-user-interface-you-need-terrascope-account",
    "title": "Introduction",
    "section": "Uploading through User Interface (you need terrascope account!)",
    "text": "Uploading through User Interface (you need terrascope account!)\nUser can upload the datasets using the RDM website, We provide a intuitive and AI assisted workflow to facilitate fast upload of user datasets. This requires terrascope login which also provides options to login using social media acccounts.\n\nStep 1: Dataset Qualification Check\nThe dataset must adhere to certain formats and contain minimum attributes. User must answer the following questions to ease the dataset upload process.\n1. Dataset should have spatial geometry.\n2. Dataset must cover years 2017 onwards.\n3. Dataset should have information on observation time.(date or season/year or year)\nIf the answer for all the above questions is “yes” then the datset is qualified to be uploaded in RDM and can used in the generation of the products. These checks are mainly to prevent errors later and facilitate easy product generation.\n\n\nStep 2: Dataset Upload Guidelines\nPrepare the dataset according to below mentioned steps to upload successfully.\n\nSupported Dataset Format.\nSupported file formats: Shapefile, GeoPackage and GeoParquet.\nCo-ordinate system should be EPSG 4326 (WGS_84 lat/lon).\nLanguage: English.\nDataset Naming.\nSelect an appropriate name for your dataset. Refer to this document for guidelines.\nWorldCereal Crop Type Legend.\nCrop type or land cover labels in your dataset need to be converted to the WorldCereal crop type legend to ensure compatibility with other datasets. You will be asked to select the dataset attribute containing the original labels. You will be guided to map these labels to the official WorldCereal crop type legend.\nDataset attribute must be String or Integer type.\nValidity Time.\nYou will be asked to select the dataset attribute that contains the observation date for each individual sample. As an alternative, you will have the possibility to define one observation date for all observations.\nDataset attribute must be Date type.\nIrrigation Status (optional).\nYou will be asked to select the dataset attribute containing information on irrigation (if present). You will be guided to map the original irrigation labels to the WorldCereal irrigation legend. Dataset attribute must be String or Integer type.\n\n\n\nStep 3: Dataset Upload\nNext step is to upload the dataset, for this we need the following basic information\n\nTitle: Full title describing the dataset.\nDataset ID: Consisting of min 3 and max 40 alphanumeric lower case characters, e.g. 2024kenyacopernicus4geoglam. For datasets to be shared with other users, we encourage to use our dataset naming convention. Note that unsupported characters will be automatically removed.\nFile: Dataset file in geoparquet (parquet) or geopackage(gpkg) or shape file (zipped)\n\nAfter the file is uploaded successfully the RDM processes the file and automatically harmonizes and adds it to community store as private dataset",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Upload Your Data"
    ]
  },
  {
    "objectID": "rdm/upload.html#automated-harmonization-and-upload",
    "href": "rdm/upload.html#automated-harmonization-and-upload",
    "title": "Introduction",
    "section": "Automated harmonization and Upload",
    "text": "Automated harmonization and Upload\n\nAttribute Extraction: In this step RDM extract all the attribute names available in the uploaded dataset except geometry. This attribute list is presented to user to choose which attribute need to be selected for crop type, irrigation(optional) and observation date mapping\nAttribute Mapping: In this step user is shown the AI based mapping done in RDM for Crop type and Irrigation. User can either accept the mapping or modify them if needed.\nHarmonization: This is last step before dataset is harmonized to worldcereal standards and assimilated into the store.\n\nThe uploaded user dataset will be availabe to user as private dataset and will not be shared with either other users or with consortium yet. To share with consortium with suitable license users can select the “share with consortium” option available in the user dataset details page.",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Upload Your Data"
    ]
  },
  {
    "objectID": "rdm/upload.html#using-your-data-in-the-processing-module",
    "href": "rdm/upload.html#using-your-data-in-the-processing-module",
    "title": "Introduction",
    "section": "Using your data in the processing module",
    "text": "Using your data in the processing module\nUsers can use the uploaded datasets to train the modules in the processing module, Refer How to Retrieve User Private Datasets in Explore page.",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Upload Your Data"
    ]
  },
  {
    "objectID": "rdm/overview.html",
    "href": "rdm/overview.html",
    "title": "Reference Data Module",
    "section": "",
    "text": "//////Jeroen comments ////// Definition of in-situ reference data for crop mapping\nTechnical background of RDM\nlist of main functionalities –&gt; refer to more detailed sections\n////// Jeroen comments //////",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Overview"
    ]
  },
  {
    "objectID": "rdm/overview.html#harmonization",
    "href": "rdm/overview.html#harmonization",
    "title": "Reference Data Module",
    "section": "Harmonization",
    "text": "Harmonization\nWorldcereal follows standardised legends and procedures to prepare harmonized datasets. Below are the list of documents explaining the different procedures.\n\nCroptype legends\nIrrigation status legends\nObservation Date\nConfidence score calculations",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Overview"
    ]
  },
  {
    "objectID": "design/processing_system.html",
    "href": "design/processing_system.html",
    "title": "openEO based processing",
    "section": "",
    "text": "The architecture of WorldCereal phase II is based upon the guidelines provided by the ESA APEx project. The key goal is to build a system and processing workflows that can be used beyond the project lifetime.\nIn accordance with the APEx framework, a few project specific choices have been made:\n\nopenEO is selected as the standard in which processing workflows are described\nCopernicus Dataspace Ecosystem openEO federation is selected as the target platform\n\n\n\nWorldCereal selects openEO as processing standard, because it allows to express both the training and inference workflows in a high level manner. While in phase I, the consortium had to invest substantially in writing data access code, this is now handled by openEO.\nThe use of openEO aids in generating results that follow FAIR data and open science principles. This is crucial to provide transparency and reproducibility of WorldCereal results. It will also make it very feasible for anyone to inspect workflows in a visual manner.\n\n\n\nThe scalable processing needed to generate a global map is offered by the CDSE openEO federation. This includes the software to manage cloud resources, the software to parallelize workflows, disaster recovery across 2 datacenters, and the operational burden of monitoring the system.\nThe CDSE open source deployment is described in more detail in the CDSE openEO documentation. The most important element is that CDSE components are required to maintain an uptime of 99.5% on a monthly basis. Lower uptimes are possible, but result in penalties for the platform operators. The contract also has a long lifetime, ensuring that WorldCereal workflows can be executed beyond the project lifetime.\nFinally, it is important to note that CDSE is the only EU platform that offers access to the full archives of Sentinel-2 L2A and Sentinel-1 GRD data. Hence, for global map production there are currently no alternatives, except for non-EU commercial cloud providers."
  },
  {
    "objectID": "design/processing_system.html#openeo-as-api",
    "href": "design/processing_system.html#openeo-as-api",
    "title": "openEO based processing",
    "section": "",
    "text": "WorldCereal selects openEO as processing standard, because it allows to express both the training and inference workflows in a high level manner. While in phase I, the consortium had to invest substantially in writing data access code, this is now handled by openEO.\nThe use of openEO aids in generating results that follow FAIR data and open science principles. This is crucial to provide transparency and reproducibility of WorldCereal results. It will also make it very feasible for anyone to inspect workflows in a visual manner."
  },
  {
    "objectID": "design/processing_system.html#cdse-as-processing-platform",
    "href": "design/processing_system.html#cdse-as-processing-platform",
    "title": "openEO based processing",
    "section": "",
    "text": "The scalable processing needed to generate a global map is offered by the CDSE openEO federation. This includes the software to manage cloud resources, the software to parallelize workflows, disaster recovery across 2 datacenters, and the operational burden of monitoring the system.\nThe CDSE open source deployment is described in more detail in the CDSE openEO documentation. The most important element is that CDSE components are required to maintain an uptime of 99.5% on a monthly basis. Lower uptimes are possible, but result in penalties for the platform operators. The contract also has a long lifetime, ensuring that WorldCereal workflows can be executed beyond the project lifetime.\nFinally, it is important to note that CDSE is the only EU platform that offers access to the full archives of Sentinel-2 L2A and Sentinel-1 GRD data. Hence, for global map production there are currently no alternatives, except for non-EU commercial cloud providers."
  },
  {
    "objectID": "design/processing_system.html#processing-system-design",
    "href": "design/processing_system.html#processing-system-design",
    "title": "openEO based processing",
    "section": "Processing system design",
    "text": "Processing system design\nThe design of the processing system is focused on 3 main use cases:\n\nGenerating maps based on existing models\nTraining new models based on a user-defined selection of reference data\nDisseminating products\n\nThese processes, and the links to various components are shown in the diagram below. Key operational components are supported by the CDSE and APEx projects, to ensure that the system remains operational beyond project lifetime.\n\n\n\nProcessing system\n\n\nThe WorldCereal Toolbox is an open source Python library and set of Jupyter notebooks, that can be used to train custom models. It offers a user-friendly API and easy example code, to make the process as easy as possible.\nThis toolbox also contains other WorldCereal specific source code that was used to develop the system, or to generate openEO UDPs. It is based on GFMap, a general purpose, openEO-based, library for mapping applications.\nThe WorldCereal openEO UDPs are user defined processes that encapsulate specific WorldCereal workflows. These workflows can be executed from generic tools such as the openEO web editor, and are hosted in APEx.\nThe APEx Algorithm hosting service maintains a catalog of openEO UDPs, ensuring that they remain discoverable beyond the project lifetime, and allowing initiatives such as the ESA Stakeholder engagement facility to promote the use of WorldCereal services. The hosting service also runs regular benchmarks and tests, to be able to flag if an algorithm is no longer functional.\nThe APEx Upscaling service allows to produce results over a larger area, based on openEO UDPs. This allows WorldCereal users to generate larger maps.\nThe STAC catalog is used for dissemination and long term archiving of maps generated by the project. An openEO backend can export results directly to STAC, or this can be performed by the WorldCereal Toolbox."
  },
  {
    "objectID": "design/user_management.html",
    "href": "design/user_management.html",
    "title": "Authentication",
    "section": "",
    "text": "All authentication in WorldCereal is done via OIDC, the most widely adopted standard at this time, and also the standard used within various openEO deployments (openEO platform, CDSE). It was also used in phase 1 of the project.\nWorldCereal does not have a requirement to maintain a user credentials database, so we opt to rely on one or more external identity providers (IdP). This results in extra features such as Single Sign-On (SSO) and enhanced security. It also reduces the operational costs, which increases sustainability after project end."
  },
  {
    "objectID": "design/user_management.html#viewing-demonstrator-datasets",
    "href": "design/user_management.html#viewing-demonstrator-datasets",
    "title": "Authentication",
    "section": "Viewing demonstrator datasets",
    "text": "Viewing demonstrator datasets\nThe WorldCereal viewer can be used anonymously, no authorization needed."
  },
  {
    "objectID": "design/user_management.html#data-dissemination",
    "href": "design/user_management.html#data-dissemination",
    "title": "Authentication",
    "section": "Data dissemination",
    "text": "Data dissemination\nGlobal demonstrator products are free and open data. Strictly speaking no authorization is needed, but dissemination and catalog services may require authentication to prevent abuse."
  },
  {
    "objectID": "design/user_management.html#custom-processing",
    "href": "design/user_management.html#custom-processing",
    "title": "Authentication",
    "section": "Custom processing",
    "text": "Custom processing\nUsers can launch jobs to train models or to generate products. This will be done via the CDSE openEO instance. In general, there’s two options here:\n\nThe user has CDSE account, and uses CDSE public service directly. This option is already possible today, as it does not require anything special.\nThe user uses the openEO UDPs, as onboarded into ESA NoR, and wants to run the service with NoR sponsoring. This option is foreseen to be supported by APEx, who wil handle integration with the (new) NoR. The IdP is still to be determined."
  },
  {
    "objectID": "design/user_management.html#reference-data-uploading",
    "href": "design/user_management.html#reference-data-uploading",
    "title": "Authentication",
    "section": "Reference data uploading",
    "text": "Reference data uploading\nUsers can upload reference data after logging in. Their reference data is either private or public, or private and shared with WorldCereal, but can not be shared with other users. This means that should simply be able to login, via the IdP. Currently Terrascope is used here as IdP. No custom authorization rules are needed.\nFor RDM power users, the RDM should allow simple configuration of a limited set of users."
  },
  {
    "objectID": "design/inference.html",
    "href": "design/inference.html",
    "title": "Crop type map production",
    "section": "",
    "text": "Generating a WorldCereal product based on an existing model basically involves running the corresponding UDP on the CDSE openEO federation. There are multiple ways to do this, depending on the user’s needs and preferences, so consulting the online documentation is the best way forward. We will however elaborate a few approaches here, to illustrate the possibilities.\nNote that all possibilities require that the user has either requested access to the service via ESA network of resources, or uses public service credits available on CDSE free of charge.\nThe WorldCereal VDM will allow to trigger processing via a web UI. This is a purpose built interface for WorldCereal, aiding the user in setting the correct parameters. Similarly, the ESA APEx portal is also expected to offer a similar capability, but based on a generic user interface. The generated results can be downloaded as Geotiff, for visualization in QGis.\nWhen the user wants to generate maps for larger areas, there is the option to use the APEx ‘upscaling service’, which is built for this purpose, or the user can resort to using Python based tools such as GFMap to run a script that generates the map. In both cases, the actual execution again happens on CDSE openEO.\nFinally, a Python notebook part of WorldCereal toolbox will show how to generate results."
  },
  {
    "objectID": "design/inference.html#general-flow",
    "href": "design/inference.html#general-flow",
    "title": "Crop type map production",
    "section": "",
    "text": "Generating a WorldCereal product based on an existing model basically involves running the corresponding UDP on the CDSE openEO federation. There are multiple ways to do this, depending on the user’s needs and preferences, so consulting the online documentation is the best way forward. We will however elaborate a few approaches here, to illustrate the possibilities.\nNote that all possibilities require that the user has either requested access to the service via ESA network of resources, or uses public service credits available on CDSE free of charge.\nThe WorldCereal VDM will allow to trigger processing via a web UI. This is a purpose built interface for WorldCereal, aiding the user in setting the correct parameters. Similarly, the ESA APEx portal is also expected to offer a similar capability, but based on a generic user interface. The generated results can be downloaded as Geotiff, for visualization in QGis.\nWhen the user wants to generate maps for larger areas, there is the option to use the APEx ‘upscaling service’, which is built for this purpose, or the user can resort to using Python based tools such as GFMap to run a script that generates the map. In both cases, the actual execution again happens on CDSE openEO.\nFinally, a Python notebook part of WorldCereal toolbox will show how to generate results."
  },
  {
    "objectID": "design/inference.html#on-premise-execution",
    "href": "design/inference.html#on-premise-execution",
    "title": "Crop type map production",
    "section": "On premise execution",
    "text": "On premise execution\nFor users that wish to generate results on their own infrastructure, a local openEO deployment will be needed.\nTo achieve this, the CDSE openEO backend software is also made available in a docker image, allowing to start an openEO application on a single node. This assumes that nodes are used with sufficient CPU and memory resources. Based on current experience, a node with 32 cores and 256GB of memory is sufficient to run the workflow, this should match with commonly available server class hardware. The development of this docker image and supporting documentation is expected to be performed in the ESA EOEPCA project.\nFor data access, the easiest option is to provide a configuration that enables remote access to the Copernicus Dataspace Ecosystem catalog and object storage. Alternatively, users can set up local STAC catalogs, that mirror relevant parts of the Copernicus Dataspace Ecosystem catalog. The local openEO deployment can then reference the local catalog and datasets. For mirroring of data, we refer to the EO-DAG tool as one option which is recommended by the ESA EOEPCA project. Note however that efficient and production-ready data mirroring is beyond the scope of WorldCereal, so is the full responsibility of the user that wishes to operate processing locally."
  },
  {
    "objectID": "design/inference.html#production-workflow",
    "href": "design/inference.html#production-workflow",
    "title": "Crop type map production",
    "section": "Production workflow",
    "text": "Production workflow\nWorldCereal products are generated by openEO workflows. The workflow requires a trained CatBoost model, which is a parameter because users may want to use their own models.\nThe pseudo code below outlines the general steps of the inference pipeline.\n\nimport  openeo\nfrom    openeo.rest.mlmodel import MlModel\nfrom    openeo.processes import ProcessBuilder\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\n1l2A = connection.load_collection(\"SENTINEL2_L2A\").aggregate_temporal_period(period=\"month\",reducer=\"mean\")\nsentinel1 = connection.load_collection(\"SENTINEL1_GRD\")\nbs = sentinel1.sar_backscatter(coefficient=\"sigma0-ellipsoid\").resample_spatial(resolution=20).aggregate_temporal_period(period=\"month\",reducer=\"mean\")\n\n2\n\nfrom    openeo import UDF\nfeature_udf=UDF(code=\"\",runtime=\"Python\") #load UDF to compute presto features based on monthly timeseries\nfeatures_cube = l2A.merge_cubes(bs).apply_dimension(dimension='t',process=feature_udf,target_dimension='bands')\n\n\nmodel = MlModel.load_ml_model(connection=connection, id=\"http://myhost.com/my_catboost_stac_metadata.json\")\n\ncatboost_classifier = lambda data, context: ProcessBuilder.process(\"predict_catboost\",data=data, model=context)\nworldcereal_product = features_cube.reduce_dimension(dimension=\"bands\", reducer=catboost_classifier, context=model)\n\nworldcereal_product\n\n\n1\n\ninstead of aggregate_temporal, we’ll do more advanced compositing, such as max-NDVI\n\n2\n\nwe’ll need to add agera5 and dem bands\n\n\n\n\n\n\n\n    \n    \n        \n    \n    \n\n\nFigure 1: The WorldCereal inference pipeline"
  },
  {
    "objectID": "design/inference.html#exporting-results-to-workspace",
    "href": "design/inference.html#exporting-results-to-workspace",
    "title": "Crop type map production",
    "section": "Exporting results to workspace",
    "text": "Exporting results to workspace\nThe openEO backend can store generated products directly in a custom object storage location. This is an optional step, but convenient when trying to avoid copying around files.\nNext to storing the file, it is also important to update and store the STAC metadata.\n\nstac_metadata = worldcereal_product.save_result(format=\"GTiff\")\nstac_metadata = connection.datacube_from_process(\"stac_update\",data = stac_metadata) #todo: add custom metadata\n\nconnection.datacube_from_process(\"export_workspace\",data = stac_metadata, workspace = \"my_workspace\", merge=\"pointer_to_worldcereal_collection\")\n\n\n\n\n    \n    \n        \n    \n    \n\n\nFigure 2: Workflow steps to export results to object storage"
  },
  {
    "objectID": "design/cloudnative_rdm.html",
    "href": "design/cloudnative_rdm.html",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "This proposal outlines a potential design for the reference data module, that is based on cloud native file formats and serverless computing.\nThis proposal should be seen as a summary of various ideas and technologies that have been proposed elsewhere (OGC 2017), (Holmes 2023), but applied to ESA WorldCereal context and the problem of managing large datasets of parcel data.\n\n\n\nThis document explores an RDM design based on some relatively recent, but mature technologies. It is not meant to immediately replace the current webservice based RDM, as there is an important risk that we will still discover issues in the design that are currently unknown, while the RDM webservice in any case already exists.\nBy providing this document, we aim to properly inform about available technologies and how they can be used. We believe this allows to gradually evolve the current setup, or to provide a complementary alternative.\n\n\n\nThis section lists a number of key design drivers (or requirements) that have been considered in this design. FAIR & open science principles are a major driver in ESA WorldCereal, but also long term operational costs are an issue.\n\n\nFor a module to be maintainable over a long time after the project, it is key to have a low operational cost. Operational costs are incurred by various factors, but a key point is that any open source component needs an update at some point. This is often triggered by the need to apply security patches, or to be able to implement small enhancements.\nAnother cost factor is of course the number of virtual machines that are needed to run the module.\n\n\n\nFAIR principle A1: (Meta)data are retrievable by their identifier using a standardised communication protocol seems easy to support by using http. The question is however how we can ensure that http links remain valid over time.\nThe ‘cloud native’ solution to this problem is to simply have static versions of data and metadata.\nSo for instance: https://worldcereal-rdm.geo-wiki.org/collections/static/2018beflandersfullpoly110.json could point to a static json file containing STAC collection metadata. It would be relatively easy to maintain such a link in the longer term.\n\n\n\nFair principle R1.3: (Meta)data meet domain-relevant community standards states that domain relevant community standards should be used to describe (meta)data.\nFor the metadata, we propose the use of the STAC standard, complemented with specific extensions where relevant. The use of STAC makes it possible to effectively find reference data that is relevant for a specific area of interest.\nFor the datasets, a real standard does not yet exist, but the GeoParquet format at least allows us to represent the geometry in a standardized manner.\nThe use of the STAC label extension would allow us to describe the legend used by worldcereal in a machine readable way.\n\n\n\nFair principle R1.2: (Meta)data are associated with detailed provenance requires that a dataset or model generated by a system like WorldCereal, can point to the source data that was used to generate it.\nThe RDM enables this by providing the reference data that was used to train models. So the STAC metadata of a model, can point to STAC metadata of reference data collections used to train the model. In STAC, this is typically done using links with a ‘derived-from’ relation type.\n\n\n\n\nThe proposal can largely be seen as a logical evolution of the current design, combined with principles from cloud native geospatial.\nThe first sentence of (OGC 2017) states: &gt; Cloud-native geospatial offers many benefits to location data users ranging from decreasing the burden on data providers, to drastically lowering the costs of managing that data\nSo the relationship with our own requirement to decrease operational costs should be clear.\nTo make this general idea more specific, we point to a few technological advancements in the past years, that are also explained in the next sections:\n\nGeoParquet as columnar, cloud-native data format.\nDuckDB: an in-memory database that allows to integrate SQL queries on Parquet files in client side scripts or the browser.\n\nThe design is also triggered by a key observation that RDM data is not considered volatile. When new data is added to the RDM, once made public, that version of the data should stay where it is, without changing. Hence we can make technology choices that are optimized for this type of data.\n\n\n\n\nGeoParquet is often referred to as a ‘columnar’ format. This section explains what this stands for and why it is relevant.\nColumnar simply refers to the fact that data is organized by column as opposed to by row. ‘Traditional’ SQL databases like PostGreSQL store data by row, which for instance allows to easily update all values in a row.\nColumnar storage was introduced to address the need for fast analytics queries. For instance, creating a histogram of all croptypes is much faster with columnar, because you only need to load a single column for such a query.\nAnother important property is that GeoParquet is ‘cloud-native’. This usually refers to the simple property of being able to load parts of the data using an HTTP ‘Range’ request. This avoids a full download of the file, when only a small part is needed. This ability can replace a key function that is often performed by web services: reading only a small part of a larger database.\nGeoParquet is also a binary format, with internal compression. This means that file sizes are much smaller compared to for instance GeoJSON. Compression is of course only applied at chunk level, to retain the property of partial reads.\n\n\n\nDuckDB is an in-memory SQL engine that can run for instance as part of a Python script, or even in a web browser. It is built for speed, and can handle large datasets. It will utilize all available cpu’s, and in our tests could perform analytical queries on RDM data very fast.\nIt support the spatial SQL extension that can also be found in PostGIS, and an H3 extension for working with H3 hexagons. This means that many of the API requests that are normally handled by a web service backed by a SQL database, can just as well be handled by DuckDB.\nThe major advantage of client-side SQL is that you don’t need a server, drastically lowering resource and maintenance cost. Another key advantage, is that users can run any SQL query they like, without needing to ask for a new API endpoint.\n\n\n\nNote that, to validate these technologies, we have effectively performed experiments using WorldCereal reference datasets. These experiments mainly confirmed the statements made above.\n\n\n\n\nBased on these key components, the high-level design is as follows:\nAll data is stored as GeoParquet. Large files are partitioned by H3 index. This allows to very quickly find the datasets that are relevant for a specific AOI. Partitioning avoids overly large files that may still become problematic when being processed in a single call.\nSTAC items can be made to point to the subfiles in a collection. This allows to use either STAC or GeoParquet partitioning to find required files.\nA STAC catalog is built to allow users to find the data they need. Both a static and dynamic catalog are relevant here. Static can be considered a ‘long term’ archive/backup that is always online. Dynamic web service based catalogs helps with discoverability and easy searching.\nNote that it’s even an option to use GeoParquet to store collection metadata as well: this allows to find the right collection very fast.\n\n\nWorldcereal has a need for user-specific collections that are not generally discoverable or accessible.\nThe most basic way in which this design supports such collections, is simply by letting those users manage the STAC and GeoParquet files themselves. There is no web service involved, so there is no need for complex access control and user management.\nThe second option is to make use of STAC collections that are not publicly accessible. This feature is being developed as part of the Terrascope STAC catalog.\nThe security scheme applied here is very important, because when a GeoParquet file is protected by a complex scheme, tools like DuckDB may not be able to support easy reading over http. For instance, a typical OIDC flow is very complex, or else requires the user to get a bearer token, and then to somehow instruct DuckDB to set an HTTP header with that token when trying to access this specific GeoParquet file over http.\nThe proposed solution would be to use url signing: https://github.com/stac-extensions/authentication?tab=readme-ov-file#url-signing This is also used by Microsoft Planetary Computer. It allows the user to generate a specific url that is valid for a limited time, which allows direct access over http.\n\n\n\nTo fully validate the design, proper integration options with other webservices should be explored. In the context of WorldCereal, services based on the openEO standard are most relevant.\nOpenEO already supports two predefined processes that may be relevant: - load_stac to load a vector cube from stac metadata - load_url to load a GeoParquet file from a url\nWhat is missing is a process that allows to filter a vector cube both on spatial region and on properties. Note that the concept of filter pushdown is very important, so the process definition should give backends the freedom to perform this type of optimization.\nWe could also consider to allow SQL style queries on vector cubes, but note that some SQL functionality does overlap with other concepts in openEO.\n\n\n\n\nThese sections validate the design in terms of specific use cases.\n\n\nThe stratification algorithm selects a subset of samples from the full RDM, to be used in model training. It is (normally) a location specific algorithm, as it is generally important to use samples that are properly spatially distributed.\nHence a geospatial index is very important, which is why we propose to add H3 indices to the geoparquet files. By partitioning large (or all) files on this index, we effectively get a data structure that is optimized for spatial queries, without requiring a more complex index.\nThe other problem is how to store the result of the algorithm. There’s a few options here:\n\nadd a column to all GeoParquet files, indicating if a sample is selected.\nCreate a new geoparquet, containing indices of selected samples, allowing to perform a join.\nCreate a new geoparquet, containing the selected subset with full information. (Or at least the information required for further processing.)\n\nIn terms of performance, option number 3 is certainly attractive. It will however come at a storage cost, which is to be evaluated. It is also possible to combine option 3 with either 1 or 2, in the sense that a temporary file can be generated, and stored for the time that it is actively used.\n\n\n\nFor the public data, the extractions are already cached (in another GeoParquet). So openEO can load a vector cube containing extractions for public samples, based on a spatial region.+\nFor user provided data, if the data is public, the caching mechanism should be triggered automatically.\nFor user provided data, if the data is private, the user should invoke a workflow that generates point based extractions. The workflow will require the (signed) url of the private GeoParquet as input, and will generate a Geoparquet with extractions.\nFor the training itself, the user loads 2 vector cubes (public and private extractions), and performs a merge_cubes before invoking the model training operation."
  },
  {
    "objectID": "design/cloudnative_rdm.html#introduction",
    "href": "design/cloudnative_rdm.html#introduction",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "This proposal outlines a potential design for the reference data module, that is based on cloud native file formats and serverless computing.\nThis proposal should be seen as a summary of various ideas and technologies that have been proposed elsewhere (OGC 2017), (Holmes 2023), but applied to ESA WorldCereal context and the problem of managing large datasets of parcel data."
  },
  {
    "objectID": "design/cloudnative_rdm.html#purpose-scope",
    "href": "design/cloudnative_rdm.html#purpose-scope",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "This document explores an RDM design based on some relatively recent, but mature technologies. It is not meant to immediately replace the current webservice based RDM, as there is an important risk that we will still discover issues in the design that are currently unknown, while the RDM webservice in any case already exists.\nBy providing this document, we aim to properly inform about available technologies and how they can be used. We believe this allows to gradually evolve the current setup, or to provide a complementary alternative."
  },
  {
    "objectID": "design/cloudnative_rdm.html#design-drivers",
    "href": "design/cloudnative_rdm.html#design-drivers",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "This section lists a number of key design drivers (or requirements) that have been considered in this design. FAIR & open science principles are a major driver in ESA WorldCereal, but also long term operational costs are an issue.\n\n\nFor a module to be maintainable over a long time after the project, it is key to have a low operational cost. Operational costs are incurred by various factors, but a key point is that any open source component needs an update at some point. This is often triggered by the need to apply security patches, or to be able to implement small enhancements.\nAnother cost factor is of course the number of virtual machines that are needed to run the module.\n\n\n\nFAIR principle A1: (Meta)data are retrievable by their identifier using a standardised communication protocol seems easy to support by using http. The question is however how we can ensure that http links remain valid over time.\nThe ‘cloud native’ solution to this problem is to simply have static versions of data and metadata.\nSo for instance: https://worldcereal-rdm.geo-wiki.org/collections/static/2018beflandersfullpoly110.json could point to a static json file containing STAC collection metadata. It would be relatively easy to maintain such a link in the longer term.\n\n\n\nFair principle R1.3: (Meta)data meet domain-relevant community standards states that domain relevant community standards should be used to describe (meta)data.\nFor the metadata, we propose the use of the STAC standard, complemented with specific extensions where relevant. The use of STAC makes it possible to effectively find reference data that is relevant for a specific area of interest.\nFor the datasets, a real standard does not yet exist, but the GeoParquet format at least allows us to represent the geometry in a standardized manner.\nThe use of the STAC label extension would allow us to describe the legend used by worldcereal in a machine readable way.\n\n\n\nFair principle R1.2: (Meta)data are associated with detailed provenance requires that a dataset or model generated by a system like WorldCereal, can point to the source data that was used to generate it.\nThe RDM enables this by providing the reference data that was used to train models. So the STAC metadata of a model, can point to STAC metadata of reference data collections used to train the model. In STAC, this is typically done using links with a ‘derived-from’ relation type."
  },
  {
    "objectID": "design/cloudnative_rdm.html#proposed-design",
    "href": "design/cloudnative_rdm.html#proposed-design",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "The proposal can largely be seen as a logical evolution of the current design, combined with principles from cloud native geospatial.\nThe first sentence of (OGC 2017) states: &gt; Cloud-native geospatial offers many benefits to location data users ranging from decreasing the burden on data providers, to drastically lowering the costs of managing that data\nSo the relationship with our own requirement to decrease operational costs should be clear.\nTo make this general idea more specific, we point to a few technological advancements in the past years, that are also explained in the next sections:\n\nGeoParquet as columnar, cloud-native data format.\nDuckDB: an in-memory database that allows to integrate SQL queries on Parquet files in client side scripts or the browser.\n\nThe design is also triggered by a key observation that RDM data is not considered volatile. When new data is added to the RDM, once made public, that version of the data should stay where it is, without changing. Hence we can make technology choices that are optimized for this type of data.\n\n\n\n\nGeoParquet is often referred to as a ‘columnar’ format. This section explains what this stands for and why it is relevant.\nColumnar simply refers to the fact that data is organized by column as opposed to by row. ‘Traditional’ SQL databases like PostGreSQL store data by row, which for instance allows to easily update all values in a row.\nColumnar storage was introduced to address the need for fast analytics queries. For instance, creating a histogram of all croptypes is much faster with columnar, because you only need to load a single column for such a query.\nAnother important property is that GeoParquet is ‘cloud-native’. This usually refers to the simple property of being able to load parts of the data using an HTTP ‘Range’ request. This avoids a full download of the file, when only a small part is needed. This ability can replace a key function that is often performed by web services: reading only a small part of a larger database.\nGeoParquet is also a binary format, with internal compression. This means that file sizes are much smaller compared to for instance GeoJSON. Compression is of course only applied at chunk level, to retain the property of partial reads.\n\n\n\nDuckDB is an in-memory SQL engine that can run for instance as part of a Python script, or even in a web browser. It is built for speed, and can handle large datasets. It will utilize all available cpu’s, and in our tests could perform analytical queries on RDM data very fast.\nIt support the spatial SQL extension that can also be found in PostGIS, and an H3 extension for working with H3 hexagons. This means that many of the API requests that are normally handled by a web service backed by a SQL database, can just as well be handled by DuckDB.\nThe major advantage of client-side SQL is that you don’t need a server, drastically lowering resource and maintenance cost. Another key advantage, is that users can run any SQL query they like, without needing to ask for a new API endpoint.\n\n\n\nNote that, to validate these technologies, we have effectively performed experiments using WorldCereal reference datasets. These experiments mainly confirmed the statements made above.\n\n\n\n\nBased on these key components, the high-level design is as follows:\nAll data is stored as GeoParquet. Large files are partitioned by H3 index. This allows to very quickly find the datasets that are relevant for a specific AOI. Partitioning avoids overly large files that may still become problematic when being processed in a single call.\nSTAC items can be made to point to the subfiles in a collection. This allows to use either STAC or GeoParquet partitioning to find required files.\nA STAC catalog is built to allow users to find the data they need. Both a static and dynamic catalog are relevant here. Static can be considered a ‘long term’ archive/backup that is always online. Dynamic web service based catalogs helps with discoverability and easy searching.\nNote that it’s even an option to use GeoParquet to store collection metadata as well: this allows to find the right collection very fast.\n\n\nWorldcereal has a need for user-specific collections that are not generally discoverable or accessible.\nThe most basic way in which this design supports such collections, is simply by letting those users manage the STAC and GeoParquet files themselves. There is no web service involved, so there is no need for complex access control and user management.\nThe second option is to make use of STAC collections that are not publicly accessible. This feature is being developed as part of the Terrascope STAC catalog.\nThe security scheme applied here is very important, because when a GeoParquet file is protected by a complex scheme, tools like DuckDB may not be able to support easy reading over http. For instance, a typical OIDC flow is very complex, or else requires the user to get a bearer token, and then to somehow instruct DuckDB to set an HTTP header with that token when trying to access this specific GeoParquet file over http.\nThe proposed solution would be to use url signing: https://github.com/stac-extensions/authentication?tab=readme-ov-file#url-signing This is also used by Microsoft Planetary Computer. It allows the user to generate a specific url that is valid for a limited time, which allows direct access over http.\n\n\n\nTo fully validate the design, proper integration options with other webservices should be explored. In the context of WorldCereal, services based on the openEO standard are most relevant.\nOpenEO already supports two predefined processes that may be relevant: - load_stac to load a vector cube from stac metadata - load_url to load a GeoParquet file from a url\nWhat is missing is a process that allows to filter a vector cube both on spatial region and on properties. Note that the concept of filter pushdown is very important, so the process definition should give backends the freedom to perform this type of optimization.\nWe could also consider to allow SQL style queries on vector cubes, but note that some SQL functionality does overlap with other concepts in openEO.\n\n\n\n\nThese sections validate the design in terms of specific use cases.\n\n\nThe stratification algorithm selects a subset of samples from the full RDM, to be used in model training. It is (normally) a location specific algorithm, as it is generally important to use samples that are properly spatially distributed.\nHence a geospatial index is very important, which is why we propose to add H3 indices to the geoparquet files. By partitioning large (or all) files on this index, we effectively get a data structure that is optimized for spatial queries, without requiring a more complex index.\nThe other problem is how to store the result of the algorithm. There’s a few options here:\n\nadd a column to all GeoParquet files, indicating if a sample is selected.\nCreate a new geoparquet, containing indices of selected samples, allowing to perform a join.\nCreate a new geoparquet, containing the selected subset with full information. (Or at least the information required for further processing.)\n\nIn terms of performance, option number 3 is certainly attractive. It will however come at a storage cost, which is to be evaluated. It is also possible to combine option 3 with either 1 or 2, in the sense that a temporary file can be generated, and stored for the time that it is actively used.\n\n\n\nFor the public data, the extractions are already cached (in another GeoParquet). So openEO can load a vector cube containing extractions for public samples, based on a spatial region.+\nFor user provided data, if the data is public, the caching mechanism should be triggered automatically.\nFor user provided data, if the data is private, the user should invoke a workflow that generates point based extractions. The workflow will require the (signed) url of the private GeoParquet as input, and will generate a Geoparquet with extractions.\nFor the training itself, the user loads 2 vector cubes (public and private extractions), and performs a merge_cubes before invoking the model training operation."
  },
  {
    "objectID": "processing/planned.html",
    "href": "processing/planned.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "&lt; empty &gt;",
    "crumbs": [
      "Processing System",
      "Planned Features and Upgrades"
    ]
  },
  {
    "objectID": "processing/supported.html",
    "href": "processing/supported.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "&lt; empty &gt;",
    "crumbs": [
      "Processing System",
      "Currently Supported Features"
    ]
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "Did not find an answer to your question on this portal?\nVisit our user forum to start a discussion!\n\n\nIn need of more details on a particular topic?\nThe WorldCereal project also organizes different courses, both online and face-to-face. All information on these courses can be found on our dedicated training website.\nOur project website also hosts links to our scientific publications and project reports.",
    "crumbs": [
      "Support and Resources"
    ]
  },
  {
    "objectID": "processing/usage_standalone.html",
    "href": "processing/usage_standalone.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "&lt; empty &gt;",
    "crumbs": [
      "Processing System",
      "Usage of the Standalone System"
    ]
  },
  {
    "objectID": "processing/overview.html",
    "href": "processing/overview.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "&lt; empty &gt;",
    "crumbs": [
      "Processing System",
      "Overview"
    ]
  },
  {
    "objectID": "processing/usage_cloud.html",
    "href": "processing/usage_cloud.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "&lt; empty &gt;",
    "crumbs": [
      "Processing System",
      "Usage of the Cloud System"
    ]
  },
  {
    "objectID": "design/extractions.html",
    "href": "design/extractions.html",
    "title": "Model training EO data management",
    "section": "",
    "text": "To effectively train models, we need very fast access to the input data at the locations where ground truth information is available. The WorldCereal classifier is primarily looking at the time-series for multiple bands per pixel. The native Sentinel earth observation data archives are not stored in a format that enables fast time series access. For instance, a Sentinel-2 product uses internal chunks of 1000x1000 pixels, while for training we need only a few 64x64 pixel chunks. Hence, a lot of unnecessary data is read from relatively slow storage when constructing such a timeseries.\nA more favourable model is thus to generate files that contain the full timeseries and all bands for a given sensor. A single read operation can then load a full year worth of EO data for a location where reference data is available.\nWhat the training step requires is analysis-ready data, which in the case of WorldCereal currently means single-pixel, cloud-free monthly composites. As there currently does not yet exist a consensus on how to best generate this analysis-ready data. The design choice was made to store the data in 2 formats:\n\nRaster files containing 64x64 pixel chunks of raw EO data observations, minimal cloud-screening.\nParquet files containing analysis-ready pixel timeseries, with all input bands and the ground-truth label.\n\nHaving these two formats enables experimentation at both preprocessing level, and the level of model tuning on ARD data. If at some point the spatial context of a pixel is taken into account, this information is also available.\nIn the sections below, this general design is detailed further. We generally refer to the locally stored EO data as ‘the extractions’, because they are extracts from the main EO archive.\n\n\n\n\nThe first level cache is a collection of netCDF raster files, all with a fixed size of e.g. 64x64 pixels.\n\n\n\n\nGet id of extraction to run\nFor extraction id get point locations from RDM\nUse UDF to convert points into 64x64 patches\n\n\n\n\n\nnetCDF assets need to link back to the sample from which they were generated.\na ‘Ground truth’ asset contains the raster with ground truth info, meaning the croptype code.\nSentinel-2 asset at 10m resolution.\nSentinel-1 asset at 20m resolution.\nAgERA5 asset\n\nSTAC extensions: - projection (proj) provides detailed info on raster size and projection system\n\n\n{\n  \"description\": \"The Level 1 input data cache contains extracted samples of EO data. It's main use is model calibration, allowing faster iterations by providing a cache.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          4.053457,\n          51.01616,\n          4.129008,\n          51.049831\n        ]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2020-05-01T00:00:00Z\",\n          \"2020-05-22T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"id\": \"L1_CACHE\",\n  \"license\": \"CC-BY-4.0\",\n  \"links\": [],\n  \"providers\": [\n    {\n      \"description\": \"This data was processed on an openEO backend maintained by VITO.\",\n      \"name\": \"VITO\",\n      \"processing:facility\": \"openEO Geotrellis backend\",\n      \"processing:software\": {\n        \"Geotrellis backend\": \"0.27.0a1\"\n      },\n      \"roles\": [\n        \"processor\"\n      ]\n    }\n  ],\n  \"stac_extensions\": [\n    \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/file/v2.1.0/schema.json\",\n    \"https://stac-extensions.github.io/processing/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\"\n  ],\n  \"stac_version\": \"1.0.0\",\n  \"summaries\": {\n    \"constellation\": [\n      \"sentinel-2\"\n    ],\n    \"instruments\": [\n      \"msi\"\n    ],\n    \"gsd\": [\n      10,\n      20,\n      60\n    ],\n    \"platform\": [\n      \"sentinel-2a\",\n      \"sentinel-2b\"\n    ]\n  },\n  \"title\": \"WorldCereal Level 1 cache\",\n  \"type\": \"Collection\",\n  \"cube:dimensions\": {\n    \"x\": {\n      \"type\": \"spatial\",\n      \"axis\": \"x\",\n      \"step\": 10,\n      \"reference_system\": {\n        \"$schema\": \"https://proj.org/schemas/v0.2/projjson.schema.json\",\n        \"area\": \"World\",\n        \"bbox\": {\n          \"east_longitude\": 180,\n          \"north_latitude\": 90,\n          \"south_latitude\": -90,\n          \"west_longitude\": -180\n        },\n        \"coordinate_system\": {\n          \"axis\": [\n            {\n              \"abbreviation\": \"Lat\",\n              \"direction\": \"north\",\n              \"name\": \"Geodetic latitude\",\n              \"unit\": \"degree\"\n            },\n            {\n              \"abbreviation\": \"Lon\",\n              \"direction\": \"east\",\n              \"name\": \"Geodetic longitude\",\n              \"unit\": \"degree\"\n            }\n          ],\n          \"subtype\": \"ellipsoidal\"\n        },\n        \"datum\": {\n          \"ellipsoid\": {\n            \"inverse_flattening\": 298.257223563,\n            \"name\": \"WGS 84\",\n            \"semi_major_axis\": 6378137\n          },\n          \"name\": \"World Geodetic System 1984\",\n          \"type\": \"GeodeticReferenceFrame\"\n        },\n        \"id\": {\n          \"authority\": \"OGC\",\n          \"code\": \"Auto42001\",\n          \"version\": \"1.3\"\n        },\n        \"name\": \"AUTO 42001 (Universal Transverse Mercator)\",\n        \"type\": \"GeodeticCRS\"\n      }\n    },\n    \"y\": {\n      \"type\": \"spatial\",\n      \"axis\": \"y\",\n      \"step\": 10,\n      \"reference_system\": {\n        \"$schema\": \"https://proj.org/schemas/v0.2/projjson.schema.json\",\n        \"area\": \"World\",\n        \"bbox\": {\n          \"east_longitude\": 180,\n          \"north_latitude\": 90,\n          \"south_latitude\": -90,\n          \"west_longitude\": -180\n        },\n        \"coordinate_system\": {\n          \"axis\": [\n            {\n              \"abbreviation\": \"Lat\",\n              \"direction\": \"north\",\n              \"name\": \"Geodetic latitude\",\n              \"unit\": \"degree\"\n            },\n            {\n              \"abbreviation\": \"Lon\",\n              \"direction\": \"east\",\n              \"name\": \"Geodetic longitude\",\n              \"unit\": \"degree\"\n            }\n          ],\n          \"subtype\": \"ellipsoidal\"\n        },\n        \"datum\": {\n          \"ellipsoid\": {\n            \"inverse_flattening\": 298.257223563,\n            \"name\": \"WGS 84\",\n            \"semi_major_axis\": 6378137\n          },\n          \"name\": \"World Geodetic System 1984\",\n          \"type\": \"GeodeticReferenceFrame\"\n        },\n        \"id\": {\n          \"authority\": \"OGC\",\n          \"code\": \"Auto42001\",\n          \"version\": \"1.3\"\n        },\n        \"name\": \"AUTO 42001 (Universal Transverse Mercator)\",\n        \"type\": \"GeodeticCRS\"\n      }\n    },\n    \"time\": {\n      \"type\": \"temporal\",\n      \"extent\": [\n        \"2015-06-23T00:00:00Z\",\n        \"2019-07-10T13:44:56Z\"\n      ],\n      \"step\": \"P5D\"\n    },\n    \"spectral\": {\n      \"type\": \"bands\",\n      \"values\": [\n        \"SCL\",\n        \"B01\",\n        \"B02\",\n        \"B03\",\n        \"B04\",\n        \"B05\",\n        \"B06\",\n        \"B07\",\n        \"B08\",\n        \"B8A\",\n        \"B09\",\n        \"B10\",\n        \"B11\",\n        \"B12\",\n        \"CROPTYPE\"\n      ]\n    }\n  },\n  \"item_assets\": {\n    \"sentinel2\": {\n      \"gsd\": 10,\n      \"title\": \"Sentinel2\",\n      \"description\": \"Sentinel-2 bands\",\n      \"type\": \"application/x-netcdf\",\n      \"roles\": [\n        \"data\"\n      ],\n      \"proj:shape\": [\n        64,\n        64\n      ],\n      \"raster:bands\": [\n        {\n          \"name\": \"B01\"\n        },\n        {\n          \"name\": \"B02\"\n        }\n      ],\n      \"cube:variables\": {\n        \"B01\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B02\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B03\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B04\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B05\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B06\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B07\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B8A\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B08\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B11\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B12\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"SCL\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"}\n      },\n      \"eo:bands\": [\n        {\n          \"name\": \"B01\",\n          \"common_name\": \"coastal\",\n          \"center_wavelength\": 0.443,\n          \"full_width_half_max\": 0.027\n        },\n        {\n          \"name\": \"B02\",\n          \"common_name\": \"blue\",\n          \"center_wavelength\": 0.49,\n          \"full_width_half_max\": 0.098\n        },\n        {\n          \"name\": \"B03\",\n          \"common_name\": \"green\",\n          \"center_wavelength\": 0.56,\n          \"full_width_half_max\": 0.045\n        },\n        {\n          \"name\": \"B04\",\n          \"common_name\": \"red\",\n          \"center_wavelength\": 0.665,\n          \"full_width_half_max\": 0.038\n        },\n        {\n          \"name\": \"B05\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.704,\n          \"full_width_half_max\": 0.019\n        },\n        {\n          \"name\": \"B06\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.74,\n          \"full_width_half_max\": 0.018\n        },\n        {\n          \"name\": \"B07\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.783,\n          \"full_width_half_max\": 0.028\n        },\n        {\n          \"name\": \"B08\",\n          \"common_name\": \"nir\",\n          \"center_wavelength\": 0.842,\n          \"full_width_half_max\": 0.145\n        },\n        {\n          \"name\": \"B8A\",\n          \"common_name\": \"nir08\",\n          \"center_wavelength\": 0.865,\n          \"full_width_half_max\": 0.033\n        },\n        {\n          \"name\": \"B11\",\n          \"common_name\": \"swir16\",\n          \"center_wavelength\": 1.61,\n          \"full_width_half_max\": 0.143\n        },\n        {\n          \"name\": \"B12\",\n          \"common_name\": \"swir22\",\n          \"center_wavelength\": 2.19,\n          \"full_width_half_max\": 0.242\n        }\n      ]\n    },\n    \"auxiliary\": {\n      \"title\": \"ground truth data\",\n      \"description\": \"This asset contains the crop type codes.\",\n      \"type\": \"application/x-netcdf\",\n      \"roles\": [\n        \"data\"\n      ],\n      \"proj:shape\": [\n        64,\n        64\n      ],\n      \"raster:bands\": [\n        {\n          \"name\": \"CROPTYPE\",\n          \"data_type\": \"uint16\",\n          \"bits_per_sample\": 16\n        }\n      ]\n    },\n    \"sentinel1\": {},\n    \"agera5\": {}\n  }\n}\n\n\n\n\nThe RDM needs to be queried for new collections on a regular basis, to discover new collections. Whenever a new collection is available in the RDM, we want the extraction workflow to automatically update the cache, allowing users to train models efficiently on all data available in the RDM.\n\n\nhttp fetch collections -&gt; DetectDuplicate/DeduplicateRecord for fast duplicate dropping -&gt; LookupRecord to check if we already know about the collection\nQuery SQL\nNew collections become flow files\nPer new collection, do job splitting.\nContinuously run job job manager on job splits.\nMonitoring: NiFi processors to send mail\n\n\n\nKubernetes can schedule cron job, allowing to run Python script on a daily basis, detecting new collections.\nMonitoring: alertmanager\nFeature required: job manager write to Parquet on S3? Feature GFMap: write to workspace Or User Workspace with http access? Or as ‘upcscaling service’ pod in k8s?\nDashboard:"
  },
  {
    "objectID": "design/extractions.html#extractions-cache",
    "href": "design/extractions.html#extractions-cache",
    "title": "Model training EO data management",
    "section": "",
    "text": "The first level cache is a collection of netCDF raster files, all with a fixed size of e.g. 64x64 pixels.\n\n\n\n\nGet id of extraction to run\nFor extraction id get point locations from RDM\nUse UDF to convert points into 64x64 patches\n\n\n\n\n\nnetCDF assets need to link back to the sample from which they were generated.\na ‘Ground truth’ asset contains the raster with ground truth info, meaning the croptype code.\nSentinel-2 asset at 10m resolution.\nSentinel-1 asset at 20m resolution.\nAgERA5 asset\n\nSTAC extensions: - projection (proj) provides detailed info on raster size and projection system\n\n\n{\n  \"description\": \"The Level 1 input data cache contains extracted samples of EO data. It's main use is model calibration, allowing faster iterations by providing a cache.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          4.053457,\n          51.01616,\n          4.129008,\n          51.049831\n        ]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2020-05-01T00:00:00Z\",\n          \"2020-05-22T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"id\": \"L1_CACHE\",\n  \"license\": \"CC-BY-4.0\",\n  \"links\": [],\n  \"providers\": [\n    {\n      \"description\": \"This data was processed on an openEO backend maintained by VITO.\",\n      \"name\": \"VITO\",\n      \"processing:facility\": \"openEO Geotrellis backend\",\n      \"processing:software\": {\n        \"Geotrellis backend\": \"0.27.0a1\"\n      },\n      \"roles\": [\n        \"processor\"\n      ]\n    }\n  ],\n  \"stac_extensions\": [\n    \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/file/v2.1.0/schema.json\",\n    \"https://stac-extensions.github.io/processing/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\"\n  ],\n  \"stac_version\": \"1.0.0\",\n  \"summaries\": {\n    \"constellation\": [\n      \"sentinel-2\"\n    ],\n    \"instruments\": [\n      \"msi\"\n    ],\n    \"gsd\": [\n      10,\n      20,\n      60\n    ],\n    \"platform\": [\n      \"sentinel-2a\",\n      \"sentinel-2b\"\n    ]\n  },\n  \"title\": \"WorldCereal Level 1 cache\",\n  \"type\": \"Collection\",\n  \"cube:dimensions\": {\n    \"x\": {\n      \"type\": \"spatial\",\n      \"axis\": \"x\",\n      \"step\": 10,\n      \"reference_system\": {\n        \"$schema\": \"https://proj.org/schemas/v0.2/projjson.schema.json\",\n        \"area\": \"World\",\n        \"bbox\": {\n          \"east_longitude\": 180,\n          \"north_latitude\": 90,\n          \"south_latitude\": -90,\n          \"west_longitude\": -180\n        },\n        \"coordinate_system\": {\n          \"axis\": [\n            {\n              \"abbreviation\": \"Lat\",\n              \"direction\": \"north\",\n              \"name\": \"Geodetic latitude\",\n              \"unit\": \"degree\"\n            },\n            {\n              \"abbreviation\": \"Lon\",\n              \"direction\": \"east\",\n              \"name\": \"Geodetic longitude\",\n              \"unit\": \"degree\"\n            }\n          ],\n          \"subtype\": \"ellipsoidal\"\n        },\n        \"datum\": {\n          \"ellipsoid\": {\n            \"inverse_flattening\": 298.257223563,\n            \"name\": \"WGS 84\",\n            \"semi_major_axis\": 6378137\n          },\n          \"name\": \"World Geodetic System 1984\",\n          \"type\": \"GeodeticReferenceFrame\"\n        },\n        \"id\": {\n          \"authority\": \"OGC\",\n          \"code\": \"Auto42001\",\n          \"version\": \"1.3\"\n        },\n        \"name\": \"AUTO 42001 (Universal Transverse Mercator)\",\n        \"type\": \"GeodeticCRS\"\n      }\n    },\n    \"y\": {\n      \"type\": \"spatial\",\n      \"axis\": \"y\",\n      \"step\": 10,\n      \"reference_system\": {\n        \"$schema\": \"https://proj.org/schemas/v0.2/projjson.schema.json\",\n        \"area\": \"World\",\n        \"bbox\": {\n          \"east_longitude\": 180,\n          \"north_latitude\": 90,\n          \"south_latitude\": -90,\n          \"west_longitude\": -180\n        },\n        \"coordinate_system\": {\n          \"axis\": [\n            {\n              \"abbreviation\": \"Lat\",\n              \"direction\": \"north\",\n              \"name\": \"Geodetic latitude\",\n              \"unit\": \"degree\"\n            },\n            {\n              \"abbreviation\": \"Lon\",\n              \"direction\": \"east\",\n              \"name\": \"Geodetic longitude\",\n              \"unit\": \"degree\"\n            }\n          ],\n          \"subtype\": \"ellipsoidal\"\n        },\n        \"datum\": {\n          \"ellipsoid\": {\n            \"inverse_flattening\": 298.257223563,\n            \"name\": \"WGS 84\",\n            \"semi_major_axis\": 6378137\n          },\n          \"name\": \"World Geodetic System 1984\",\n          \"type\": \"GeodeticReferenceFrame\"\n        },\n        \"id\": {\n          \"authority\": \"OGC\",\n          \"code\": \"Auto42001\",\n          \"version\": \"1.3\"\n        },\n        \"name\": \"AUTO 42001 (Universal Transverse Mercator)\",\n        \"type\": \"GeodeticCRS\"\n      }\n    },\n    \"time\": {\n      \"type\": \"temporal\",\n      \"extent\": [\n        \"2015-06-23T00:00:00Z\",\n        \"2019-07-10T13:44:56Z\"\n      ],\n      \"step\": \"P5D\"\n    },\n    \"spectral\": {\n      \"type\": \"bands\",\n      \"values\": [\n        \"SCL\",\n        \"B01\",\n        \"B02\",\n        \"B03\",\n        \"B04\",\n        \"B05\",\n        \"B06\",\n        \"B07\",\n        \"B08\",\n        \"B8A\",\n        \"B09\",\n        \"B10\",\n        \"B11\",\n        \"B12\",\n        \"CROPTYPE\"\n      ]\n    }\n  },\n  \"item_assets\": {\n    \"sentinel2\": {\n      \"gsd\": 10,\n      \"title\": \"Sentinel2\",\n      \"description\": \"Sentinel-2 bands\",\n      \"type\": \"application/x-netcdf\",\n      \"roles\": [\n        \"data\"\n      ],\n      \"proj:shape\": [\n        64,\n        64\n      ],\n      \"raster:bands\": [\n        {\n          \"name\": \"B01\"\n        },\n        {\n          \"name\": \"B02\"\n        }\n      ],\n      \"cube:variables\": {\n        \"B01\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B02\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B03\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B04\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B05\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B06\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B07\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B8A\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B08\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B11\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B12\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"SCL\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"}\n      },\n      \"eo:bands\": [\n        {\n          \"name\": \"B01\",\n          \"common_name\": \"coastal\",\n          \"center_wavelength\": 0.443,\n          \"full_width_half_max\": 0.027\n        },\n        {\n          \"name\": \"B02\",\n          \"common_name\": \"blue\",\n          \"center_wavelength\": 0.49,\n          \"full_width_half_max\": 0.098\n        },\n        {\n          \"name\": \"B03\",\n          \"common_name\": \"green\",\n          \"center_wavelength\": 0.56,\n          \"full_width_half_max\": 0.045\n        },\n        {\n          \"name\": \"B04\",\n          \"common_name\": \"red\",\n          \"center_wavelength\": 0.665,\n          \"full_width_half_max\": 0.038\n        },\n        {\n          \"name\": \"B05\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.704,\n          \"full_width_half_max\": 0.019\n        },\n        {\n          \"name\": \"B06\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.74,\n          \"full_width_half_max\": 0.018\n        },\n        {\n          \"name\": \"B07\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.783,\n          \"full_width_half_max\": 0.028\n        },\n        {\n          \"name\": \"B08\",\n          \"common_name\": \"nir\",\n          \"center_wavelength\": 0.842,\n          \"full_width_half_max\": 0.145\n        },\n        {\n          \"name\": \"B8A\",\n          \"common_name\": \"nir08\",\n          \"center_wavelength\": 0.865,\n          \"full_width_half_max\": 0.033\n        },\n        {\n          \"name\": \"B11\",\n          \"common_name\": \"swir16\",\n          \"center_wavelength\": 1.61,\n          \"full_width_half_max\": 0.143\n        },\n        {\n          \"name\": \"B12\",\n          \"common_name\": \"swir22\",\n          \"center_wavelength\": 2.19,\n          \"full_width_half_max\": 0.242\n        }\n      ]\n    },\n    \"auxiliary\": {\n      \"title\": \"ground truth data\",\n      \"description\": \"This asset contains the crop type codes.\",\n      \"type\": \"application/x-netcdf\",\n      \"roles\": [\n        \"data\"\n      ],\n      \"proj:shape\": [\n        64,\n        64\n      ],\n      \"raster:bands\": [\n        {\n          \"name\": \"CROPTYPE\",\n          \"data_type\": \"uint16\",\n          \"bits_per_sample\": 16\n        }\n      ]\n    },\n    \"sentinel1\": {},\n    \"agera5\": {}\n  }\n}\n\n\n\n\nThe RDM needs to be queried for new collections on a regular basis, to discover new collections. Whenever a new collection is available in the RDM, we want the extraction workflow to automatically update the cache, allowing users to train models efficiently on all data available in the RDM.\n\n\nhttp fetch collections -&gt; DetectDuplicate/DeduplicateRecord for fast duplicate dropping -&gt; LookupRecord to check if we already know about the collection\nQuery SQL\nNew collections become flow files\nPer new collection, do job splitting.\nContinuously run job job manager on job splits.\nMonitoring: NiFi processors to send mail\n\n\n\nKubernetes can schedule cron job, allowing to run Python script on a daily basis, detecting new collections.\nMonitoring: alertmanager\nFeature required: job manager write to Parquet on S3? Feature GFMap: write to workspace Or User Workspace with http access? Or as ‘upcscaling service’ pod in k8s?\nDashboard:"
  },
  {
    "objectID": "design/model_training.html",
    "href": "design/model_training.html",
    "title": "Training custom models",
    "section": "",
    "text": "The user should be able to train models based on custom reference data. The preprocessing and feature computation approach remain the same as for standard model, but the model is simply retrained. This functionality will be offered in the form of a Python API, and supported by Jupyter notebooks, as part of the WorldCereal Toolbox component.\nModel training is also performed using openEO workflows. In principle, the full workflow could work from scratch, but in practice there’s a need to store and cache intermediate results. This reduces the cost of model training when multiple iterations are needed.\nThe subsequent sections describe the various steps involved in model training."
  },
  {
    "objectID": "design/model_training.html#preprocessing-features",
    "href": "design/model_training.html#preprocessing-features",
    "title": "Training custom models",
    "section": "Preprocessing features",
    "text": "Preprocessing features\nPreprocessing aim is to generate a 2D data structure (a table) that can go into catboost training.\n\nSampling point locations\nThe WorldCereal extractions cache consists of 64x64 pixel timeseries stored as netCDF files. As catboost is a 1D method, we need to sample those patches at point locations.\nIn the approach presented in the code block below, the original algorithm is translated into an openEO process graph. It is however also possible to come up with other approaches, for instance that sample the patches at point locations, and then perform a stratification step on the larger dataset.\n\nimport  openeo\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\nground_truth = connection.load_stac(\"https://stac_catalog.com/ground_truth\")\n\nfrom   openeo import UDF\n1sampling_udf=UDF(code=\"\",runtime=\"Python\")\n\npolygons = {\"type\":\"FeatureCollection\"} #these would be the bounding boxes of the netCDF files, or in fact STAC item bboxes\n\nground_truth.apply_polygon(polygons,process=sampling_udf)\n\n\n1\n\nThis UDF should return points as geojson\n\n\n\n\n\n    \n    \n        \n    \n    \n\n\n\n\nExtracting point timeseries\n\nimport  openeo\nfrom    openeo.rest.mlmodel import MlModel\nfrom    openeo.processes import ProcessBuilder\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\n1l2A = connection.load_stac(\"https://stac_catalog.com/SENTINEL2_L2A\").aggregate_temporal_period(period=\"month\",reducer=\"mean\")\nsentinel1 = connection.load_stac(\"https://stac_catalog.com/SENTINEL1_BS\")\nbs = sentinel1.aggregate_temporal_period(period=\"month\",reducer=\"mean\")\n\ntimesteps_cube = l2A.merge_cubes(bs).aggregate_spatial(geometries={\"type\":\"Point\"},reducer=\"mean\").save_result(format=\"Parquet\")\n2\n\ntimesteps_cube\n\n\n1\n\ninstead of aggregate_temporal, we’ll do more advanced compositing, such as max-NDVI\n\n2\n\nwe’ll need to add agera5 and dem bands\n\n\n\n\n\n    \n    \n        \n    \n    \n\n\n\n\nTraining workflow\nThe training workflow combines feature computation starting from monthly timesteps with catboost training.\nThe output is a model together with STAC metadata. The link to the STAC metadata of the model can be used by an inference workflow.\n\nfrom    openeo import UDF\nfeature_udf=UDF(code=\"\",runtime=\"Python\") #load UDF to compute presto features based on monthly timeseries\nfeatures_cube = connection.load_url(\"timesteps.parquet\",format=\"Parquet\").apply_dimension(dimension='t',process=feature_udf,target_dimension='bands')\nml_model = features_cube.process(\"fit_catboost_model\", data=features_cube)\nml_model"
  },
  {
    "objectID": "design/model_training.html#extracting-private-samples",
    "href": "design/model_training.html#extracting-private-samples",
    "title": "Training custom models",
    "section": "Extracting private samples",
    "text": "Extracting private samples\nFor this use case, we assume that the user wants to use a private reference dataset. It should be available at a ‘secret’ url, which can be a signed url provided by the reference data module. Multiple input formats are supported by openEO next to GeoParquet, but the input data needs to be harmonized.\nWe immediately extract a table at point locations, assuming that the cache of intermediate patches has less value for private data.\nThe WorldCereal preprocessing chain is assumed to be available as an openEO User Defined Process (UDP) called worldcereal_preprocessing_udp.\n\nsample_locations = connection.load_url(\"https://rdm.worldcereal.org/private_assets/absqdfjq_signed_url/private_data.parquet\", format=\"Parquet\")\n\nconnection.datacube_from_process(\"worldcereal_preprocessing_udp\").aggregate_spatial(sample_locations,reducer=\"first\").save_result(format=\"Parquet\")"
  },
  {
    "objectID": "design/model_training.html#training-by-combining-private-public-samples",
    "href": "design/model_training.html#training-by-combining-private-public-samples",
    "title": "Training custom models",
    "section": "Training by combining private + public samples",
    "text": "Training by combining private + public samples\nIn this usecase, the user wants to train a new model, by combining data. This should be possible by simply merging vector cubes that go into the training process.\nTODO"
  },
  {
    "objectID": "design/reference_data_module.html",
    "href": "design/reference_data_module.html",
    "title": "Reference Data Management",
    "section": "",
    "text": "While the RDM is built on top of a PostGIS database, we more and more start to use H3 for fast geospatial lookup. H3 is a discrete global grid system, with some nice properties for cases like ours.\nOne example use case is creating a heatmap showing distribution of samples for specific crop types over the world.\nH3 indexes can also be computed on the fly from the geometry, but a lot of operations can be made faster if they are performed on 64bit integers rather than geometries.\n\n\nThis can be achieved in different technology stacks. For instance, in (postgres)[https://github.com/zachasme/h3-pg]:\nSELECT h3_lat_lng_to_cell(POINT('37.3615593,-122.0553238'), 5)\nor in (Python)[https://uber.github.io/h3-py/intro.html#usage]:\nh3.latlng_to_cell(lat, lng, resolution)\nor other options: https://h3geo.org/docs/community/bindings"
  },
  {
    "objectID": "design/reference_data_module.html#reading-test-from-parquet",
    "href": "design/reference_data_module.html#reading-test-from-parquet",
    "title": "Reference Data Management",
    "section": "Reading test from Parquet",
    "text": "Reading test from Parquet\n\n%%time\nimport geopandas as gpd\nimport fsspec\npq_path = \"https://ewocstorage.blob.core.windows.net/collections/2021_PT_EUROCROP_POLY_110.parquet\"\nwith fsspec.open(pq_path) as file:\n    df = gpd.read_parquet(file,columns=[\"geometry\",\"CT\"])\ndf\n\nCPU times: user 909 ms, sys: 305 ms, total: 1.21 s\nWall time: 17.3 s\n\n\n\n\n\n\n\n\n\ngeometry\nCT\n\n\n\n\n0\nMULTIPOLYGON (((-8.54796 40.56554, -8.54942 40...\n1200\n\n\n1\nMULTIPOLYGON (((-8.52352 40.55686, -8.52352 40...\n1700\n\n\n2\nMULTIPOLYGON (((-8.52456 40.55538, -8.52454 40...\n3300\n\n\n3\nMULTIPOLYGON (((-8.52835 40.56835, -8.52837 40...\n2000\n\n\n4\nMULTIPOLYGON (((-8.52781 40.57128, -8.52814 40...\n0\n\n\n...\n...\n...\n\n\n99995\nMULTIPOLYGON (((-6.46866 41.43669, -6.46832 41...\n0\n\n\n99996\nMULTIPOLYGON (((-6.46797 41.43497, -6.46778 41...\n0\n\n\n99997\nMULTIPOLYGON (((-7.45134 41.73074, -7.45134 41...\n1200\n\n\n99998\nMULTIPOLYGON (((-6.47482 41.44217, -6.47480 41...\n0\n\n\n99999\nMULTIPOLYGON (((-6.47503 41.44182, -6.47504 41...\n9520\n\n\n\n\n100000 rows × 2 columns\n\n\n\n\ndf.groupby(['CT']).count()\n\n\n\n\n\n\n\n\ngeometry\n\n\nCT\n\n\n\n\n\n0\n37673\n\n\n1100\n191\n\n\n1200\n4475\n\n\n1300\n271\n\n\n1500\n69\n\n\n...\n...\n\n\n9300\n10\n\n\n9320\n3\n\n\n9500\n8\n\n\n9520\n971\n\n\n9920\n4\n\n\n\n\n61 rows × 1 columns\n\n\n\n\n%%time\nimport duckdb\ndb = duckdb.connect()\ndb.execute('select count(*) from read_parquet(\"https://ewocstorage.blob.core.windows.net/collections/2021_PT_EUROCROP_POLY_110.parquet\")').fetchall()\n\nCPU times: user 255 ms, sys: 4.75 ms, total: 260 ms\nWall time: 294 ms\n\n\n[(100000,)]\n\n\n\n%%time \ndb.query('select CT,count(*) from read_parquet(\"https://ewocstorage.blob.core.windows.net/collections/2021_PT_EUROCROP_POLY_110.parquet\") GROUP BY CT').to_df()\n\nCPU times: user 48.1 ms, sys: 17.3 ms, total: 65.3 ms\nWall time: 597 ms\n\n\n\n\n\n\n\n\n\nCT\ncount_star()\n\n\n\n\n0\n9100\n21039\n\n\n1\n2000\n1976\n\n\n2\n3530\n153\n\n\n3\n7100\n188\n\n\n4\n4380\n11\n\n\n...\n...\n...\n\n\n56\n1200\n4475\n\n\n57\n9520\n971\n\n\n58\n7300\n16\n\n\n59\n2190\n1\n\n\n60\n3490\n4\n\n\n\n\n61 rows × 2 columns\n\n\n\n\ndb.execute('INSTALL spatial;LOAD spatial;')\n\n&lt;duckdb.duckdb.DuckDBPyConnection at 0x70e94825ff30&gt;\n\n\n\n%%time\ndb.query('select ST_centroid(ST_GeomFromWKB(geometry)) from read_parquet(\"https://ewocstorage.blob.core.windows.net/collections/2021_PT_EUROCROP_POLY_110.parquet\") USING SAMPLE 100 ROWS ').to_df()\n\n\n\n\nCPU times: user 4.88 s, sys: 92.6 ms, total: 4.97 s\nWall time: 11.8 s\n\n\n\n\n\n\n\n\n\nst_centroid(st_geomfromwkb(geometry))\n\n\n\n\n0\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n1\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n2\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n3\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n4\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n...\n...\n\n\n95\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n96\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n97\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n98\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n99\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n\n\n100 rows × 1 columns\n\n\n\n\nGeoParquet vs OGC Features\nAs shown above, both DuckDB and GeoPandas can efficiently handle Parquet files of 100k items stored on https. With parquet as interface, data scientists can write complex queries in a language they know (Pandas, SQL, …).\nWhen looking at OGC features, it seems there are hardly any libraries available. Some basic support in GDAL seems to be the best option to connect with it. To benefit from server side processing power, the most comfortable option seems to write CQL filters. OGC Features does not support aggregation, so a ‘group by’ operation is not supported.\nNote: GeoParquet is on track to be adopted as an OGC standard, hence satisfies standardization requirements."
  },
  {
    "objectID": "rdm/explore.html",
    "href": "rdm/explore.html",
    "title": "Introduction",
    "section": "",
    "text": "The WorldCereal Reference Data Module (RDM) is a outcome of collaborative approach to build a reference data store on label data which can be used for crop and irrigation related model training and validation. The products generated support worldwide crop monitoring. In the RDM we have two types of data storage:\n\nConsortium data store\nCommunity data store\n\nData sets in both data stores can be set to be publicly available or private. Data sets that are inside the consortium data storage have been collected, harmonized, and maintained by expert moderators (project partners) and have been made available to the public according their governing data licenses. Private datasets in consortium datastore will be available only for products generation.\nThe Data sets that are in the Community data storage are harmonized and uploaded by Community users. The uploaded data sets can be made public with appropriate licenses and will be reviewed by moderators before being published. Private user data sets will be available only for the owner of such data sets and will not be shared for use by other users or consortium for product generation until the owner decides to make the data public. The owner decides who will be allowed to use data and under what restrictions.\nUser can choose from below license types.\n\n\n\nLicense types*\n\n\nRemarks\n\n\n\n\nCC0\n\n\nNo Rights Reserved\n\n\n\n\nCC BY\n\n\nAttribution\n\n\n\n\nCC BY-SA\n\n\nAttribution-ShareAlike\n\n\n\n\nCC BY-NC\n\n\nAttribution-NonCommercial\n\n\n\n\nCC BY-NC-SA\n\n\nAttribution-NonCommercial-ShareAlike\n\n\n\n\nPrivate\n\n\nOnly accessible for the owner\n\n\n\n\nOther\n\n\nTo be defined by the owner\n\n\n\n\nSee Creative Commons licenses\n\n\n\n\n\n\nWorldCereal RDM quick tutorial\n\n\nClick to watch video in youtube\n\n\n\nRDM provides REST APIs to access data. To access public datasets no user login is required. Below python notebook demonstrates how to access public datasets along with search functionatities \n\n\n\nTo access user uploaded private datasets through APIs, credentials are required. These credential are the same as were used during upload of the datasets. Below python notebook demonstrates how to get user private datasets",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Explore and Retrieve Data"
    ]
  },
  {
    "objectID": "rdm/explore.html#explore-user-interface",
    "href": "rdm/explore.html#explore-user-interface",
    "title": "Introduction",
    "section": "",
    "text": "WorldCereal RDM quick tutorial\n\n\nClick to watch video in youtube",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Explore and Retrieve Data"
    ]
  },
  {
    "objectID": "rdm/explore.html#how-to-retrieve-public-datasets",
    "href": "rdm/explore.html#how-to-retrieve-public-datasets",
    "title": "Introduction",
    "section": "",
    "text": "RDM provides REST APIs to access data. To access public datasets no user login is required. Below python notebook demonstrates how to access public datasets along with search functionatities",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Explore and Retrieve Data"
    ]
  },
  {
    "objectID": "rdm/explore.html#how-to-retrieve-user-private-datasets",
    "href": "rdm/explore.html#how-to-retrieve-user-private-datasets",
    "title": "Introduction",
    "section": "",
    "text": "To access user uploaded private datasets through APIs, credentials are required. These credential are the same as were used during upload of the datasets. Below python notebook demonstrates how to get user private datasets",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Explore and Retrieve Data"
    ]
  },
  {
    "objectID": "rdm/publish.html",
    "href": "rdm/publish.html",
    "title": "Publishing Reference Data (through moderator)",
    "section": "",
    "text": "Users can contribute to worldcereal reference data by sharing their datasets with the consortium.\nWorldCereal supports the general movement towards data sharing and open science. Please check the below link to learn more about WorldCereal’s view on opening reference data to society (https://esa-worldcereal.org/en/situ-data-global-crop-mapping).\n\n\nThis workflow is under development, In the meantime if you would like to contribute to consortium, please send a mail to ewoc-rdm@iiasa.ac.at",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Publish Your Data"
    ]
  },
  {
    "objectID": "rdm/publish.html#workflow-is-under-development",
    "href": "rdm/publish.html#workflow-is-under-development",
    "title": "Publishing Reference Data (through moderator)",
    "section": "",
    "text": "This workflow is under development, In the meantime if you would like to contribute to consortium, please send a mail to ewoc-rdm@iiasa.ac.at",
    "crumbs": [
      "Reference Data Module (RDM)",
      "Publish Your Data"
    ]
  },
  {
    "objectID": "usage/processing_api.html",
    "href": "usage/processing_api.html",
    "title": "openEO based processing",
    "section": "",
    "text": "This section explains advanced usage of the processing APIs. It explains how to work with Python and HTTP REST based APIs to run WorldCereal processing. This is meant for developers looking to integrate these workflows into their own applications.\nThe WorldCereal system makes use of the Copernicus Dataspace Ecosystem openEO API. This use of this is fully documented on the CDSE website.\nWe refer to this documentation for more advanced questions and for getting a basic understanding of openEO, but we also include concrete examples on this page, to avoid learning the full openEO API, which is quite extensive.\nMore specifically, WorldCereal workflows are offered as openEO user defined processes, that should be invoked via batch jobs.\n\n\nOpenEO provides client libraries to support the creation and execution of JavaScript, Python and R services. The full client libraries documentation is available on the official OpenEO support pages: * JavaScript * Python * R\nThe following example shows a code sample on how to execute a WorldCereal workflow through the OpenEO Python Client.\nimport openeo\n\n# Setup parameters\nspatial_extent = {\"west\":664000, \"south\":5611134, \"east\": 684000, \"north\":5631134, \"crs\":\"EPSG:32631\"}\ntemporal_extent = [\"2020-11-01\",\"2021-10-31\"]\n\n# Setup connection with OpenEO\neoconn = openeo.connect(\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n# Create a processing graph from the worldcereal-inference process using an active openEO connection\ncropmap = eoconn.datacube_from_process(\n    \"worldcereal_inference\", \n    namespace=\"https://github.com/ESA-APEx/apex_algorithms/raw/main/openeo_udp/worldcereal_inference.json\", \n    spatial_extent=spatial_extent,\n    temporal_extent=temporal_extent)\n\n# Execute the OpenEO request as a batch job\njob_options =  { \n    \"driver-memory\": \"4g\",\n    \"executor-memory\": \"1g\",\n    \"executor-memoryOverhead\": \"1g\",\n    \"python-memory\": \"2g\",\n    \"udf-dependency-archives\": [\"https://artifactory.vgt.vito.be/artifactory/auxdata-public/openeo/onnx_dependencies_1.16.3.zip#onnx_deps\"]\n}\njob = cropmap.save_result(\"GTiff\").create_job(title=\"worldcereal inference demo\",job_options=job_options)\njob.start()\nThe job has two main parameters:\n\nspatial_extent is a bounding box definition, which should be below 400km² (20x20km)\ntemporal_extent is a list of two dates, spanning exactly 1 year. The end date should correspond to the end of the growing season in the area of interest.\n\nOnce the job has started, you can use the interface available at https://openeo.dataspace.copernicus.eu to monitor it, or use the Python API.\n\n\n\nOpenEO provides a fully documented REST API. The API can also be used to integrate WorldCereal jobs into your application if the client libraries are not suitable. The documentation is available at:\n\n\n\nOpenEO\n\n\n\n\nhttps://openeo.org/documentation/1.0/developers/api/reference.html\n\n\n\nThe following example showcases how to use the OpenEO API to execute a request for a WorldCereal service:\nPOST /openeo/1.2/jobs HTTP/1.1\nHost: openeo.dataspace.copernicus.eu\nContent-Type: application/json\nAuthorization: Bearer basic//basic.cHJvag==\nContent-Length: 4587\n{\n    \"title\": \"Your custom WorldCereal job\",\n    \"process\": {\n        \"id\": \"cropmap\",\n        \"process_graph\": {\n            \"biopar1\": {\n                \"process_id\": \"worldcereal_inference\",\n                \"namespace\": \"https://github.com/ESA-APEx/apex_algorithms/raw/main/openeo_udp/worldcereal_inference.json\",\n                \"arguments\": {\n                    \"spatial_extent\": {\n                        \"west\": 5.15183687210083,\n                        \"east\": 5.153381824493408,\n                        \"south\": 51.18192559252128,\n                        \"north\": 51.18469636040683,\n                        \"crs\": \"EPSG:4326\"\n                    },\n                    \"temporal_extent\": [\n                        \"2020-11-01\",\n                        \"2021-10-31\"\n                    ]\n                },\n                \"result\": true\n            }\n        }\n    },\n    \"driver-memory\": \"4g\",\n    \"executor-memory\": \"1g\",\n    \"executor-memoryOverhead\": \"1g\",\n    \"python-memory\": \"2g\",\n    \"udf-dependency-archives\": [\n        \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/openeo/onnx_dependencies_1.16.3.zip#onnx_deps\"\n    ]\n}\nThe response of this call will be a json object containing an ‘id’ property. This property is the job id that you can use in the subsequent call to start the job:\nPOST /openeo/1.2/jobs/{job_id}/results HTTP/1.1\nHost: openeo.dataspace.copernicus.eu\nAuthorization: Bearer basic//basic.cHJvag==\nContent-Length: 0\n\nThis call will return 202 if successfully. Now you can use other parts of the API to monitor the job, request logs, result metadata or data itself.\n\n\n\nThe cost of a run for a 20x20km should be around 100 credits, but may fluctuate by up to 20 credits depending on the input data available in the processing area. Please report unexpected fluctuations, as this may indicate a problem with the processing."
  },
  {
    "objectID": "usage/processing_api.html#client-libraries",
    "href": "usage/processing_api.html#client-libraries",
    "title": "openEO based processing",
    "section": "",
    "text": "OpenEO provides client libraries to support the creation and execution of JavaScript, Python and R services. The full client libraries documentation is available on the official OpenEO support pages: * JavaScript * Python * R\nThe following example shows a code sample on how to execute a WorldCereal workflow through the OpenEO Python Client.\nimport openeo\n\n# Setup parameters\nspatial_extent = {\"west\":664000, \"south\":5611134, \"east\": 684000, \"north\":5631134, \"crs\":\"EPSG:32631\"}\ntemporal_extent = [\"2020-11-01\",\"2021-10-31\"]\n\n# Setup connection with OpenEO\neoconn = openeo.connect(\"openeofed.dataspace.copernicus.eu\").authenticate_oidc()\n\n# Create a processing graph from the worldcereal-inference process using an active openEO connection\ncropmap = eoconn.datacube_from_process(\n    \"worldcereal_inference\", \n    namespace=\"https://github.com/ESA-APEx/apex_algorithms/raw/main/openeo_udp/worldcereal_inference.json\", \n    spatial_extent=spatial_extent,\n    temporal_extent=temporal_extent)\n\n# Execute the OpenEO request as a batch job\njob_options =  { \n    \"driver-memory\": \"4g\",\n    \"executor-memory\": \"1g\",\n    \"executor-memoryOverhead\": \"1g\",\n    \"python-memory\": \"2g\",\n    \"udf-dependency-archives\": [\"https://artifactory.vgt.vito.be/artifactory/auxdata-public/openeo/onnx_dependencies_1.16.3.zip#onnx_deps\"]\n}\njob = cropmap.save_result(\"GTiff\").create_job(title=\"worldcereal inference demo\",job_options=job_options)\njob.start()\nThe job has two main parameters:\n\nspatial_extent is a bounding box definition, which should be below 400km² (20x20km)\ntemporal_extent is a list of two dates, spanning exactly 1 year. The end date should correspond to the end of the growing season in the area of interest.\n\nOnce the job has started, you can use the interface available at https://openeo.dataspace.copernicus.eu to monitor it, or use the Python API."
  },
  {
    "objectID": "usage/processing_api.html#rest-api",
    "href": "usage/processing_api.html#rest-api",
    "title": "openEO based processing",
    "section": "",
    "text": "OpenEO provides a fully documented REST API. The API can also be used to integrate WorldCereal jobs into your application if the client libraries are not suitable. The documentation is available at:\n\n\n\nOpenEO\n\n\n\n\nhttps://openeo.org/documentation/1.0/developers/api/reference.html\n\n\n\nThe following example showcases how to use the OpenEO API to execute a request for a WorldCereal service:\nPOST /openeo/1.2/jobs HTTP/1.1\nHost: openeo.dataspace.copernicus.eu\nContent-Type: application/json\nAuthorization: Bearer basic//basic.cHJvag==\nContent-Length: 4587\n{\n    \"title\": \"Your custom WorldCereal job\",\n    \"process\": {\n        \"id\": \"cropmap\",\n        \"process_graph\": {\n            \"biopar1\": {\n                \"process_id\": \"worldcereal_inference\",\n                \"namespace\": \"https://github.com/ESA-APEx/apex_algorithms/raw/main/openeo_udp/worldcereal_inference.json\",\n                \"arguments\": {\n                    \"spatial_extent\": {\n                        \"west\": 5.15183687210083,\n                        \"east\": 5.153381824493408,\n                        \"south\": 51.18192559252128,\n                        \"north\": 51.18469636040683,\n                        \"crs\": \"EPSG:4326\"\n                    },\n                    \"temporal_extent\": [\n                        \"2020-11-01\",\n                        \"2021-10-31\"\n                    ]\n                },\n                \"result\": true\n            }\n        }\n    },\n    \"driver-memory\": \"4g\",\n    \"executor-memory\": \"1g\",\n    \"executor-memoryOverhead\": \"1g\",\n    \"python-memory\": \"2g\",\n    \"udf-dependency-archives\": [\n        \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/openeo/onnx_dependencies_1.16.3.zip#onnx_deps\"\n    ]\n}\nThe response of this call will be a json object containing an ‘id’ property. This property is the job id that you can use in the subsequent call to start the job:\nPOST /openeo/1.2/jobs/{job_id}/results HTTP/1.1\nHost: openeo.dataspace.copernicus.eu\nAuthorization: Bearer basic//basic.cHJvag==\nContent-Length: 0\n\nThis call will return 202 if successfully. Now you can use other parts of the API to monitor the job, request logs, result metadata or data itself."
  },
  {
    "objectID": "usage/processing_api.html#cost-estimate",
    "href": "usage/processing_api.html#cost-estimate",
    "title": "openEO based processing",
    "section": "",
    "text": "The cost of a run for a 20x20km should be around 100 credits, but may fluctuate by up to 20 credits depending on the input data available in the processing area. Please report unexpected fluctuations, as this may indicate a problem with the processing."
  },
  {
    "objectID": "vdm/visualize.html",
    "href": "vdm/visualize.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "&lt; empty &gt;",
    "crumbs": [
      "Visualization and Dissemination Module (VDM)",
      "Visualizing Global Products"
    ]
  },
  {
    "objectID": "vdm/planned.html",
    "href": "vdm/planned.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "&lt; empty &gt;",
    "crumbs": [
      "Visualization and Dissemination Module (VDM)",
      "Planned Features and Upgrades"
    ]
  },
  {
    "objectID": "intro/purpose_scope.html",
    "href": "intro/purpose_scope.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "Having access to reliable cropland and crop type masks is an important prerequisite for any agricultural monitoring activity, e.g. - crop yield estimation - damage assessment of (natural) disasters - evaluation of agricultural practices - assessement of crop water consumption\nAs such, cropland and crop type masks have been identified as one of the key Essential Agricultural Variables by the GEOGLAM community.\nBy the end of 2026, the WorldCereal project will generate new global cropland and crop type products for a single year, covering the following crop types: maize, winter cereals, spring cereals, sunflower, rapeseed, sorghum, millet, wheat, barley, rye and soybean.\nThe ultimate goal of the WorldCereal project is to make cropland and crop type mapping more accessible to the broader agricultural community by providing a flexible, open, user-friendly and cloud-based processing system for global cropland monitoring at high resolution.\nThe processing system will allow anyone to: 1. Consult the existing global cropland and crop type products, as generated by the WorldCereal project; 2. Retrieve open and harmonized reference data on land cover and crop types for use in your own applications; 3. Upload, harmonize and share own reference data on land cover and crop types; 4. Generate cropland masks for a custom area and growing season, based on our pre-trained cropland model; 5. Generate crop type maps for a custom area and growing season, using our pre-trained crop type models (default models are only available for the eleven supported crop types); 6. Train and apply custom cropland and crop type models for any area and growing season of interest. Custom models are not restricted in terms of cropland definition, nor available crop types. To train custom models, users are able to combine their own reference data with our public reference data.\nPlease note that the current version of the system does not provide default crop type models as these are still under development. Also note that the default cropland model is continuously being improved as more reference data is being added to the system during the project lifetime. Finally, the current version of the system does not yet allow users to train models based on their own reference data.\nAs opposed to the first phase of the WorldCereal project, irrigation classification is NOT part of the processing system.",
    "crumbs": [
      "Introduction",
      "Purpose and Scope"
    ]
  },
  {
    "objectID": "intro/system_overview.html",
    "href": "intro/system_overview.html",
    "title": "An overview of the WorldCereal system",
    "section": "",
    "text": "An overview of the WorldCereal system\n\n\n\nHigh-level overview of the different modules of the WorldCereal processing system and how they are interlinked.",
    "crumbs": [
      "Introduction",
      "System Overview"
    ]
  }
]