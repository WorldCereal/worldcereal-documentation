[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESA WorldCereal documentation portal",
    "section": "",
    "text": "ESA WorldCereal documentation portal\nThis portal documents the design of the WorldCereal system.\nCode samples in this portal are meant to document the design, but in general are not working. As openEO code is relatively concise, they serve as workflow descriptions and to render workflow diagrams in the form of openEO process graphs.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "design/user_management.html",
    "href": "design/user_management.html",
    "title": "Authentication",
    "section": "",
    "text": "All authentication in WorldCereal is done via OIDC, the most widely adopted standard at this time, and also the standard used within various openEO deployments (openEO platform, CDSE). It was also used in phase 1 of the project.\nWorldCereal does not have a requirement to maintain a user credentials database, so we opt to rely on one or more external identity providers (IdP). This results in extra features such as Single Sign-On (SSO) and enhanced security. It also reduces the operational costs, which increases sustainability after project end.",
    "crumbs": [
      "System design",
      "User Management"
    ]
  },
  {
    "objectID": "design/user_management.html#viewing-demonstrator-datasets",
    "href": "design/user_management.html#viewing-demonstrator-datasets",
    "title": "Authentication",
    "section": "Viewing demonstrator datasets",
    "text": "Viewing demonstrator datasets\nThe WorldCereal viewer can be used anonymously, no authorization needed.",
    "crumbs": [
      "System design",
      "User Management"
    ]
  },
  {
    "objectID": "design/user_management.html#data-dissemination",
    "href": "design/user_management.html#data-dissemination",
    "title": "Authentication",
    "section": "Data dissemination",
    "text": "Data dissemination\nGlobal demonstrator products are free and open data. Strictly speaking no authorization is needed, but dissemination and catalog services may require authentication to prevent abuse.",
    "crumbs": [
      "System design",
      "User Management"
    ]
  },
  {
    "objectID": "design/user_management.html#custom-processing",
    "href": "design/user_management.html#custom-processing",
    "title": "Authentication",
    "section": "Custom processing",
    "text": "Custom processing\nUsers can launch jobs to train models or to generate products. This will be done via the CDSE openEO instance. In general, there’s two options here:\n\nThe user has CDSE account, and uses CDSE public service directly. This option is already possible today, as it does not require anything special.\nThe user uses the openEO UDPs, as onboarded into ESA NoR, and wants to run the service with NoR sponsoring. This option is foreseen to be supported by APEx, who wil handle integration with the (new) NoR. The IdP is still to be determined.",
    "crumbs": [
      "System design",
      "User Management"
    ]
  },
  {
    "objectID": "design/user_management.html#reference-data-uploading",
    "href": "design/user_management.html#reference-data-uploading",
    "title": "Authentication",
    "section": "Reference data uploading",
    "text": "Reference data uploading\nUsers can upload reference data after logging in. Their reference data is either private or public, or private and shared with WorldCereal, but can not be shared with other users. This means that should simply be able to login, via the IdP. Currently Terrascope is used here as IdP. No custom authorization rules are needed.\nFor RDM power users, the RDM should allow simple configuration of a limited set of users.",
    "crumbs": [
      "System design",
      "User Management"
    ]
  },
  {
    "objectID": "design/extractions.html",
    "href": "design/extractions.html",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "The first level cache is a collection of netCDF raster files, all with a fixed size of e.g. 64x64 pixels.\n\n\n\n\nGet id of extraction to run\nFor extraction id get point locations from RDM\nUse UDF to convert points into 64x64 patches\n\n\n\n\n\nnetCDF assets need to link back to the sample from which they were generated.\na ‘Ground truth’ asset contains the raster with ground truth info, meaning the croptype code.\nSentinel-2 asset at 10m resolution.\nSentinel-1 asset at 20m resolution.\nAgERA5 asset\n\nSTAC extensions: - projection (proj) provides detailed info on raster size and projection system\n\n\n{\n  \"description\": \"The Level 1 input data cache contains extracted samples of EO data. It's main use is model calibration, allowing faster iterations by providing a cache.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          4.053457,\n          51.01616,\n          4.129008,\n          51.049831\n        ]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2020-05-01T00:00:00Z\",\n          \"2020-05-22T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"id\": \"L1_CACHE\",\n  \"license\": \"CC-BY-4.0\",\n  \"links\": [],\n  \"providers\": [\n    {\n      \"description\": \"This data was processed on an openEO backend maintained by VITO.\",\n      \"name\": \"VITO\",\n      \"processing:facility\": \"openEO Geotrellis backend\",\n      \"processing:software\": {\n        \"Geotrellis backend\": \"0.27.0a1\"\n      },\n      \"roles\": [\n        \"processor\"\n      ]\n    }\n  ],\n  \"stac_extensions\": [\n    \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/file/v2.1.0/schema.json\",\n    \"https://stac-extensions.github.io/processing/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\"\n  ],\n  \"stac_version\": \"1.0.0\",\n  \"summaries\": {\n    \"constellation\": [\n      \"sentinel-2\"\n    ],\n    \"instruments\": [\n      \"msi\"\n    ],\n    \"gsd\": [\n      10,\n      20,\n      60\n    ],\n    \"platform\": [\n      \"sentinel-2a\",\n      \"sentinel-2b\"\n    ]\n  },\n  \"title\": \"WorldCereal Level 1 cache\",\n  \"type\": \"Collection\",\n  \"cube:dimensions\": {\n    \"x\": {\n      \"type\": \"spatial\",\n      \"axis\": \"x\",\n      \"step\": 10,\n      \"reference_system\": {\n        \"$schema\": \"https://proj.org/schemas/v0.2/projjson.schema.json\",\n        \"area\": \"World\",\n        \"bbox\": {\n          \"east_longitude\": 180,\n          \"north_latitude\": 90,\n          \"south_latitude\": -90,\n          \"west_longitude\": -180\n        },\n        \"coordinate_system\": {\n          \"axis\": [\n            {\n              \"abbreviation\": \"Lat\",\n              \"direction\": \"north\",\n              \"name\": \"Geodetic latitude\",\n              \"unit\": \"degree\"\n            },\n            {\n              \"abbreviation\": \"Lon\",\n              \"direction\": \"east\",\n              \"name\": \"Geodetic longitude\",\n              \"unit\": \"degree\"\n            }\n          ],\n          \"subtype\": \"ellipsoidal\"\n        },\n        \"datum\": {\n          \"ellipsoid\": {\n            \"inverse_flattening\": 298.257223563,\n            \"name\": \"WGS 84\",\n            \"semi_major_axis\": 6378137\n          },\n          \"name\": \"World Geodetic System 1984\",\n          \"type\": \"GeodeticReferenceFrame\"\n        },\n        \"id\": {\n          \"authority\": \"OGC\",\n          \"code\": \"Auto42001\",\n          \"version\": \"1.3\"\n        },\n        \"name\": \"AUTO 42001 (Universal Transverse Mercator)\",\n        \"type\": \"GeodeticCRS\"\n      }\n    },\n    \"y\": {\n      \"type\": \"spatial\",\n      \"axis\": \"y\",\n      \"step\": 10,\n      \"reference_system\": {\n        \"$schema\": \"https://proj.org/schemas/v0.2/projjson.schema.json\",\n        \"area\": \"World\",\n        \"bbox\": {\n          \"east_longitude\": 180,\n          \"north_latitude\": 90,\n          \"south_latitude\": -90,\n          \"west_longitude\": -180\n        },\n        \"coordinate_system\": {\n          \"axis\": [\n            {\n              \"abbreviation\": \"Lat\",\n              \"direction\": \"north\",\n              \"name\": \"Geodetic latitude\",\n              \"unit\": \"degree\"\n            },\n            {\n              \"abbreviation\": \"Lon\",\n              \"direction\": \"east\",\n              \"name\": \"Geodetic longitude\",\n              \"unit\": \"degree\"\n            }\n          ],\n          \"subtype\": \"ellipsoidal\"\n        },\n        \"datum\": {\n          \"ellipsoid\": {\n            \"inverse_flattening\": 298.257223563,\n            \"name\": \"WGS 84\",\n            \"semi_major_axis\": 6378137\n          },\n          \"name\": \"World Geodetic System 1984\",\n          \"type\": \"GeodeticReferenceFrame\"\n        },\n        \"id\": {\n          \"authority\": \"OGC\",\n          \"code\": \"Auto42001\",\n          \"version\": \"1.3\"\n        },\n        \"name\": \"AUTO 42001 (Universal Transverse Mercator)\",\n        \"type\": \"GeodeticCRS\"\n      }\n    },\n    \"time\": {\n      \"type\": \"temporal\",\n      \"extent\": [\n        \"2015-06-23T00:00:00Z\",\n        \"2019-07-10T13:44:56Z\"\n      ],\n      \"step\": \"P5D\"\n    },\n    \"spectral\": {\n      \"type\": \"bands\",\n      \"values\": [\n        \"SCL\",\n        \"B01\",\n        \"B02\",\n        \"B03\",\n        \"B04\",\n        \"B05\",\n        \"B06\",\n        \"B07\",\n        \"B08\",\n        \"B8A\",\n        \"B09\",\n        \"B10\",\n        \"B11\",\n        \"B12\",\n        \"CROPTYPE\"\n      ]\n    }\n  },\n  \"item_assets\": {\n    \"sentinel2\": {\n      \"gsd\": 10,\n      \"title\": \"Sentinel2\",\n      \"description\": \"Sentinel-2 bands\",\n      \"type\": \"application/x-netcdf\",\n      \"roles\": [\n        \"data\"\n      ],\n      \"proj:shape\": [\n        64,\n        64\n      ],\n      \"raster:bands\": [\n        {\n          \"name\": \"B01\"\n        },\n        {\n          \"name\": \"B02\"\n        }\n      ],\n      \"cube:variables\": {\n        \"B01\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B02\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B03\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B04\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B05\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B06\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B07\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B8A\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B08\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B11\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B12\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"SCL\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"}\n      },\n      \"eo:bands\": [\n        {\n          \"name\": \"B01\",\n          \"common_name\": \"coastal\",\n          \"center_wavelength\": 0.443,\n          \"full_width_half_max\": 0.027\n        },\n        {\n          \"name\": \"B02\",\n          \"common_name\": \"blue\",\n          \"center_wavelength\": 0.49,\n          \"full_width_half_max\": 0.098\n        },\n        {\n          \"name\": \"B03\",\n          \"common_name\": \"green\",\n          \"center_wavelength\": 0.56,\n          \"full_width_half_max\": 0.045\n        },\n        {\n          \"name\": \"B04\",\n          \"common_name\": \"red\",\n          \"center_wavelength\": 0.665,\n          \"full_width_half_max\": 0.038\n        },\n        {\n          \"name\": \"B05\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.704,\n          \"full_width_half_max\": 0.019\n        },\n        {\n          \"name\": \"B06\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.74,\n          \"full_width_half_max\": 0.018\n        },\n        {\n          \"name\": \"B07\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.783,\n          \"full_width_half_max\": 0.028\n        },\n        {\n          \"name\": \"B08\",\n          \"common_name\": \"nir\",\n          \"center_wavelength\": 0.842,\n          \"full_width_half_max\": 0.145\n        },\n        {\n          \"name\": \"B8A\",\n          \"common_name\": \"nir08\",\n          \"center_wavelength\": 0.865,\n          \"full_width_half_max\": 0.033\n        },\n        {\n          \"name\": \"B11\",\n          \"common_name\": \"swir16\",\n          \"center_wavelength\": 1.61,\n          \"full_width_half_max\": 0.143\n        },\n        {\n          \"name\": \"B12\",\n          \"common_name\": \"swir22\",\n          \"center_wavelength\": 2.19,\n          \"full_width_half_max\": 0.242\n        }\n      ]\n    },\n    \"auxiliary\": {\n      \"title\": \"ground truth data\",\n      \"description\": \"This asset contains the crop type codes.\",\n      \"type\": \"application/x-netcdf\",\n      \"roles\": [\n        \"data\"\n      ],\n      \"proj:shape\": [\n        64,\n        64\n      ],\n      \"raster:bands\": [\n        {\n          \"name\": \"CROPTYPE\",\n          \"data_type\": \"uint16\",\n          \"bits_per_sample\": 16\n        }\n      ]\n    },\n    \"sentinel1\": {},\n    \"agera5\": {}\n  }\n}\n\n\n\n\nThe RDM needs to be queried for new collections on a regular basis, to discover new collections.\n\n\nhttp fetch collections -&gt; DetectDuplicate/DeduplicateRecord for fast duplicate dropping -&gt; LookupRecord to check if we already know about the collection\nQuery SQL\nNew collections become flow files\nPer new collection, do job splitting.\nContinuously run job job manager on job splits.\nMonitoring: NiFi processors to send mail\n\n\n\nKubernetes can schedule cron job, allowing to run Python script on a daily basis, detecting new collections.\nMonitoring: alertmanager\nFeature required: job manager write to Parquet on S3? Feature GFMap: write to workspace Or User Workspace with http access? Or as ‘upcscaling service’ pod in k8s?\nDashboard:",
    "crumbs": [
      "System design",
      "Extractions"
    ]
  },
  {
    "objectID": "design/extractions.html#extractions-cache",
    "href": "design/extractions.html#extractions-cache",
    "title": "WorldCereal Documentation",
    "section": "",
    "text": "The first level cache is a collection of netCDF raster files, all with a fixed size of e.g. 64x64 pixels.\n\n\n\n\nGet id of extraction to run\nFor extraction id get point locations from RDM\nUse UDF to convert points into 64x64 patches\n\n\n\n\n\nnetCDF assets need to link back to the sample from which they were generated.\na ‘Ground truth’ asset contains the raster with ground truth info, meaning the croptype code.\nSentinel-2 asset at 10m resolution.\nSentinel-1 asset at 20m resolution.\nAgERA5 asset\n\nSTAC extensions: - projection (proj) provides detailed info on raster size and projection system\n\n\n{\n  \"description\": \"The Level 1 input data cache contains extracted samples of EO data. It's main use is model calibration, allowing faster iterations by providing a cache.\",\n  \"extent\": {\n    \"spatial\": {\n      \"bbox\": [\n        [\n          4.053457,\n          51.01616,\n          4.129008,\n          51.049831\n        ]\n      ]\n    },\n    \"temporal\": {\n      \"interval\": [\n        [\n          \"2020-05-01T00:00:00Z\",\n          \"2020-05-22T00:00:00Z\"\n        ]\n      ]\n    }\n  },\n  \"id\": \"L1_CACHE\",\n  \"license\": \"CC-BY-4.0\",\n  \"links\": [],\n  \"providers\": [\n    {\n      \"description\": \"This data was processed on an openEO backend maintained by VITO.\",\n      \"name\": \"VITO\",\n      \"processing:facility\": \"openEO Geotrellis backend\",\n      \"processing:software\": {\n        \"Geotrellis backend\": \"0.27.0a1\"\n      },\n      \"roles\": [\n        \"processor\"\n      ]\n    }\n  ],\n  \"stac_extensions\": [\n    \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/file/v2.1.0/schema.json\",\n    \"https://stac-extensions.github.io/processing/v1.1.0/schema.json\",\n    \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\"\n  ],\n  \"stac_version\": \"1.0.0\",\n  \"summaries\": {\n    \"constellation\": [\n      \"sentinel-2\"\n    ],\n    \"instruments\": [\n      \"msi\"\n    ],\n    \"gsd\": [\n      10,\n      20,\n      60\n    ],\n    \"platform\": [\n      \"sentinel-2a\",\n      \"sentinel-2b\"\n    ]\n  },\n  \"title\": \"WorldCereal Level 1 cache\",\n  \"type\": \"Collection\",\n  \"cube:dimensions\": {\n    \"x\": {\n      \"type\": \"spatial\",\n      \"axis\": \"x\",\n      \"step\": 10,\n      \"reference_system\": {\n        \"$schema\": \"https://proj.org/schemas/v0.2/projjson.schema.json\",\n        \"area\": \"World\",\n        \"bbox\": {\n          \"east_longitude\": 180,\n          \"north_latitude\": 90,\n          \"south_latitude\": -90,\n          \"west_longitude\": -180\n        },\n        \"coordinate_system\": {\n          \"axis\": [\n            {\n              \"abbreviation\": \"Lat\",\n              \"direction\": \"north\",\n              \"name\": \"Geodetic latitude\",\n              \"unit\": \"degree\"\n            },\n            {\n              \"abbreviation\": \"Lon\",\n              \"direction\": \"east\",\n              \"name\": \"Geodetic longitude\",\n              \"unit\": \"degree\"\n            }\n          ],\n          \"subtype\": \"ellipsoidal\"\n        },\n        \"datum\": {\n          \"ellipsoid\": {\n            \"inverse_flattening\": 298.257223563,\n            \"name\": \"WGS 84\",\n            \"semi_major_axis\": 6378137\n          },\n          \"name\": \"World Geodetic System 1984\",\n          \"type\": \"GeodeticReferenceFrame\"\n        },\n        \"id\": {\n          \"authority\": \"OGC\",\n          \"code\": \"Auto42001\",\n          \"version\": \"1.3\"\n        },\n        \"name\": \"AUTO 42001 (Universal Transverse Mercator)\",\n        \"type\": \"GeodeticCRS\"\n      }\n    },\n    \"y\": {\n      \"type\": \"spatial\",\n      \"axis\": \"y\",\n      \"step\": 10,\n      \"reference_system\": {\n        \"$schema\": \"https://proj.org/schemas/v0.2/projjson.schema.json\",\n        \"area\": \"World\",\n        \"bbox\": {\n          \"east_longitude\": 180,\n          \"north_latitude\": 90,\n          \"south_latitude\": -90,\n          \"west_longitude\": -180\n        },\n        \"coordinate_system\": {\n          \"axis\": [\n            {\n              \"abbreviation\": \"Lat\",\n              \"direction\": \"north\",\n              \"name\": \"Geodetic latitude\",\n              \"unit\": \"degree\"\n            },\n            {\n              \"abbreviation\": \"Lon\",\n              \"direction\": \"east\",\n              \"name\": \"Geodetic longitude\",\n              \"unit\": \"degree\"\n            }\n          ],\n          \"subtype\": \"ellipsoidal\"\n        },\n        \"datum\": {\n          \"ellipsoid\": {\n            \"inverse_flattening\": 298.257223563,\n            \"name\": \"WGS 84\",\n            \"semi_major_axis\": 6378137\n          },\n          \"name\": \"World Geodetic System 1984\",\n          \"type\": \"GeodeticReferenceFrame\"\n        },\n        \"id\": {\n          \"authority\": \"OGC\",\n          \"code\": \"Auto42001\",\n          \"version\": \"1.3\"\n        },\n        \"name\": \"AUTO 42001 (Universal Transverse Mercator)\",\n        \"type\": \"GeodeticCRS\"\n      }\n    },\n    \"time\": {\n      \"type\": \"temporal\",\n      \"extent\": [\n        \"2015-06-23T00:00:00Z\",\n        \"2019-07-10T13:44:56Z\"\n      ],\n      \"step\": \"P5D\"\n    },\n    \"spectral\": {\n      \"type\": \"bands\",\n      \"values\": [\n        \"SCL\",\n        \"B01\",\n        \"B02\",\n        \"B03\",\n        \"B04\",\n        \"B05\",\n        \"B06\",\n        \"B07\",\n        \"B08\",\n        \"B8A\",\n        \"B09\",\n        \"B10\",\n        \"B11\",\n        \"B12\",\n        \"CROPTYPE\"\n      ]\n    }\n  },\n  \"item_assets\": {\n    \"sentinel2\": {\n      \"gsd\": 10,\n      \"title\": \"Sentinel2\",\n      \"description\": \"Sentinel-2 bands\",\n      \"type\": \"application/x-netcdf\",\n      \"roles\": [\n        \"data\"\n      ],\n      \"proj:shape\": [\n        64,\n        64\n      ],\n      \"raster:bands\": [\n        {\n          \"name\": \"B01\"\n        },\n        {\n          \"name\": \"B02\"\n        }\n      ],\n      \"cube:variables\": {\n        \"B01\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B02\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B03\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B04\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B05\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B06\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B07\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B8A\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B08\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B11\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"B12\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"},\n        \"SCL\": {\"dimensions\": [\"time\",\"y\",\"x\"],\"type\": \"data\"}\n      },\n      \"eo:bands\": [\n        {\n          \"name\": \"B01\",\n          \"common_name\": \"coastal\",\n          \"center_wavelength\": 0.443,\n          \"full_width_half_max\": 0.027\n        },\n        {\n          \"name\": \"B02\",\n          \"common_name\": \"blue\",\n          \"center_wavelength\": 0.49,\n          \"full_width_half_max\": 0.098\n        },\n        {\n          \"name\": \"B03\",\n          \"common_name\": \"green\",\n          \"center_wavelength\": 0.56,\n          \"full_width_half_max\": 0.045\n        },\n        {\n          \"name\": \"B04\",\n          \"common_name\": \"red\",\n          \"center_wavelength\": 0.665,\n          \"full_width_half_max\": 0.038\n        },\n        {\n          \"name\": \"B05\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.704,\n          \"full_width_half_max\": 0.019\n        },\n        {\n          \"name\": \"B06\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.74,\n          \"full_width_half_max\": 0.018\n        },\n        {\n          \"name\": \"B07\",\n          \"common_name\": \"rededge\",\n          \"center_wavelength\": 0.783,\n          \"full_width_half_max\": 0.028\n        },\n        {\n          \"name\": \"B08\",\n          \"common_name\": \"nir\",\n          \"center_wavelength\": 0.842,\n          \"full_width_half_max\": 0.145\n        },\n        {\n          \"name\": \"B8A\",\n          \"common_name\": \"nir08\",\n          \"center_wavelength\": 0.865,\n          \"full_width_half_max\": 0.033\n        },\n        {\n          \"name\": \"B11\",\n          \"common_name\": \"swir16\",\n          \"center_wavelength\": 1.61,\n          \"full_width_half_max\": 0.143\n        },\n        {\n          \"name\": \"B12\",\n          \"common_name\": \"swir22\",\n          \"center_wavelength\": 2.19,\n          \"full_width_half_max\": 0.242\n        }\n      ]\n    },\n    \"auxiliary\": {\n      \"title\": \"ground truth data\",\n      \"description\": \"This asset contains the crop type codes.\",\n      \"type\": \"application/x-netcdf\",\n      \"roles\": [\n        \"data\"\n      ],\n      \"proj:shape\": [\n        64,\n        64\n      ],\n      \"raster:bands\": [\n        {\n          \"name\": \"CROPTYPE\",\n          \"data_type\": \"uint16\",\n          \"bits_per_sample\": 16\n        }\n      ]\n    },\n    \"sentinel1\": {},\n    \"agera5\": {}\n  }\n}\n\n\n\n\nThe RDM needs to be queried for new collections on a regular basis, to discover new collections.\n\n\nhttp fetch collections -&gt; DetectDuplicate/DeduplicateRecord for fast duplicate dropping -&gt; LookupRecord to check if we already know about the collection\nQuery SQL\nNew collections become flow files\nPer new collection, do job splitting.\nContinuously run job job manager on job splits.\nMonitoring: NiFi processors to send mail\n\n\n\nKubernetes can schedule cron job, allowing to run Python script on a daily basis, detecting new collections.\nMonitoring: alertmanager\nFeature required: job manager write to Parquet on S3? Feature GFMap: write to workspace Or User Workspace with http access? Or as ‘upcscaling service’ pod in k8s?\nDashboard:",
    "crumbs": [
      "System design",
      "Extractions"
    ]
  },
  {
    "objectID": "design/model_training.html",
    "href": "design/model_training.html",
    "title": "Training custom models",
    "section": "",
    "text": "The user should be able to train models based on custom reference data. The preprocessing and feature computation approach remain the same as for standard model, but the model is simply retrained. This functionality will be offered in the form of a Python API, and supported by Jupyter notebooks, as part of the WorldCereal Toolbox component.\nModel training is also performed using openEO workflows. In principle, the full workflow could work from scratch, but in practice there’s a need to store and cache intermediate results. This reduces the cost of model training when multiple iterations are needed.\nThe subsequent sections describe the various steps involved in model training.",
    "crumbs": [
      "System design",
      "Training"
    ]
  },
  {
    "objectID": "design/model_training.html#preprocessing-features",
    "href": "design/model_training.html#preprocessing-features",
    "title": "Training custom models",
    "section": "Preprocessing features",
    "text": "Preprocessing features\nPreprocessing aim is to generate a 2D data structure (a table) that can go into catboost training.\n\nSampling point locations\nThe WorldCereal extractions cache consists of 64x64 pixel timeseries stored as netCDF files. As catboost is a 1D method, we need to sample those patches at point locations.\nIn the approach presented in the code block below, the original algorithm is translated into an openEO process graph. It is however also possible to come up with other approaches, for instance that sample the patches at point locations, and then perform a stratification step on the larger dataset.\n\nimport  openeo\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\nground_truth = connection.load_stac(\"https://stac_catalog.com/ground_truth\")\n\nfrom   openeo import UDF\n1sampling_udf=UDF(code=\"\",runtime=\"Python\")\n\npolygons = {\"type\":\"FeatureCollection\"} #these would be the bounding boxes of the netCDF files, or in fact STAC item bboxes\n\nground_truth.apply_polygon(polygons,process=sampling_udf)\n\n\n1\n\nThis UDF should return points as geojson\n\n\n\n\n\n    \n    \n        \n    \n    \n\n\n\n\nExtracting point timeseries\n\nimport  openeo\nfrom    openeo.rest.mlmodel import MlModel\nfrom    openeo.processes import ProcessBuilder\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\n1l2A = connection.load_stac(\"https://stac_catalog.com/SENTINEL2_L2A\").aggregate_temporal_period(period=\"month\",reducer=\"mean\")\nsentinel1 = connection.load_stac(\"https://stac_catalog.com/SENTINEL1_BS\")\nbs = sentinel1.aggregate_temporal_period(period=\"month\",reducer=\"mean\")\n\ntimesteps_cube = l2A.merge_cubes(bs).aggregate_spatial(geometries={\"type\":\"Point\"},reducer=\"mean\").save_result(format=\"Parquet\")\n2\n\ntimesteps_cube\n\n\n1\n\ninstead of aggregate_temporal, we’ll do more advanced compositing, such as max-NDVI\n\n2\n\nwe’ll need to add agera5 and dem bands\n\n\n\n\n\n    \n    \n        \n    \n    \n\n\n\n\nTraining workflow\nThe training workflow combines feature computation starting from monthly timesteps with catboost training.\nThe output is a model together with STAC metadata. The link to the STAC metadata of the model can be used by an inference workflow.\n\nfrom    openeo import UDF\nfeature_udf=UDF(code=\"\",runtime=\"Python\") #load UDF to compute presto features based on monthly timeseries\nfeatures_cube = connection.load_url(\"timesteps.parquet\",format=\"Parquet\").apply_dimension(dimension='t',process=feature_udf,target_dimension='bands')\nml_model = features_cube.process(\"fit_catboost_model\", data=features_cube)\nml_model",
    "crumbs": [
      "System design",
      "Training"
    ]
  },
  {
    "objectID": "design/model_training.html#extracting-private-samples",
    "href": "design/model_training.html#extracting-private-samples",
    "title": "Training custom models",
    "section": "Extracting private samples",
    "text": "Extracting private samples\nFor this use case, we assume that the user wants to use a private reference dataset. It should be available at a ‘secret’ url, which can be a signed url provided by the reference data module. Multiple input formats are supported by openEO next to GeoParquet, but the input data needs to be harmonized.\nWe immediately extract a table at point locations, assuming that the cache of intermediate patches has less value for private data.\nThe WorldCereal preprocessing chain is assumed to be available as an openEO User Defined Process (UDP) called worldcereal_preprocessing_udp.\n\nsample_locations = connection.load_url(\"https://rdm.worldcereal.org/private_assets/absqdfjq_signed_url/private_data.parquet\", format=\"Parquet\")\n\nconnection.datacube_from_process(\"worldcereal_preprocessing_udp\").aggregate_spatial(sample_locations,reducer=\"first\").save_result(format=\"Parquet\")",
    "crumbs": [
      "System design",
      "Training"
    ]
  },
  {
    "objectID": "design/model_training.html#training-by-combining-private-public-samples",
    "href": "design/model_training.html#training-by-combining-private-public-samples",
    "title": "Training custom models",
    "section": "Training by combining private + public samples",
    "text": "Training by combining private + public samples\nIn this usecase, the user wants to train a new model, by combining data. This should be possible by simply merging vector cubes that go into the training process.\nTODO",
    "crumbs": [
      "System design",
      "Training"
    ]
  },
  {
    "objectID": "design/cloudnative_rdm.html",
    "href": "design/cloudnative_rdm.html",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "This proposal outlines a potential design for the reference data module, that is based on cloud native file formats and serverless computing.\nThis proposal should be seen as a summary of various ideas and technologies that have been proposed elsewhere (OGC 2017), (Holmes 2023), but applied to ESA WorldCereal context and the problem of managing large datasets of parcel data.\n\n\n\nThis document explores an RDM design based on some relatively recent, but mature technologies. It is not meant to immediately replace the current webservice based RDM, as there is an important risk that we will still discover issues in the design that are currently unknown, while the RDM webservice in any case already exists.\nBy providing this document, we aim to properly inform about available technologies and how they can be used. We believe this allows to gradually evolve the current setup, or to provide a complementary alternative.\n\n\n\nThis section lists a number of key design drivers (or requirements) that have been considered in this design. FAIR & open science principles are a major driver in ESA WorldCereal, but also long term operational costs are an issue.\n\n\nFor a module to be maintainable over a long time after the project, it is key to have a low operational cost. Operational costs are incurred by various factors, but a key point is that any open source component needs an update at some point. This is often triggered by the need to apply security patches, or to be able to implement small enhancements.\nAnother cost factor is of course the number of virtual machines that are needed to run the module.\n\n\n\nFAIR principle A1: (Meta)data are retrievable by their identifier using a standardised communication protocol seems easy to support by using http. The question is however how we can ensure that http links remain valid over time.\nThe ‘cloud native’ solution to this problem is to simply have static versions of data and metadata.\nSo for instance: https://worldcereal-rdm.geo-wiki.org/collections/static/2018beflandersfullpoly110.json could point to a static json file containing STAC collection metadata. It would be relatively easy to maintain such a link in the longer term.\n\n\n\nFair principle R1.3: (Meta)data meet domain-relevant community standards states that domain relevant community standards should be used to describe (meta)data.\nFor the metadata, we propose the use of the STAC standard, complemented with specific extensions where relevant. The use of STAC makes it possible to effectively find reference data that is relevant for a specific area of interest.\nFor the datasets, a real standard does not yet exist, but the GeoParquet format at least allows us to represent the geometry in a standardized manner.\nThe use of the STAC label extension would allow us to describe the legend used by worldcereal in a machine readable way.\n\n\n\nFair principle R1.2: (Meta)data are associated with detailed provenance requires that a dataset or model generated by a system like WorldCereal, can point to the source data that was used to generate it.\nThe RDM enables this by providing the reference data that was used to train models. So the STAC metadata of a model, can point to STAC metadata of reference data collections used to train the model. In STAC, this is typically done using links with a ‘derived-from’ relation type.\n\n\n\n\nThe proposal can largely be seen as a logical evolution of the current design, combined with principles from cloud native geospatial.\nThe first sentence of (OGC 2017) states: &gt; Cloud-native geospatial offers many benefits to location data users ranging from decreasing the burden on data providers, to drastically lowering the costs of managing that data\nSo the relationship with our own requirement to decrease operational costs should be clear.\nTo make this general idea more specific, we point to a few technological advancements in the past years, that are also explained in the next sections:\n\nGeoParquet as columnar, cloud-native data format.\nDuckDB: an in-memory database that allows to integrate SQL queries on Parquet files in client side scripts or the browser.\n\nThe design is also triggered by a key observation that RDM data is not considered volatile. When new data is added to the RDM, once made public, that version of the data should stay where it is, without changing. Hence we can make technology choices that are optimized for this type of data.\n\n\n\n\nGeoParquet is often referred to as a ‘columnar’ format. This section explains what this stands for and why it is relevant.\nColumnar simply refers to the fact that data is organized by column as opposed to by row. ‘Traditional’ SQL databases like PostGreSQL store data by row, which for instance allows to easily update all values in a row.\nColumnar storage was introduced to address the need for fast analytics queries. For instance, creating a histogram of all croptypes is much faster with columnar, because you only need to load a single column for such a query.\nAnother important property is that GeoParquet is ‘cloud-native’. This usually refers to the simple property of being able to load parts of the data using an HTTP ‘Range’ request. This avoids a full download of the file, when only a small part is needed. This ability can replace a key function that is often performed by web services: reading only a small part of a larger database.\nGeoParquet is also a binary format, with internal compression. This means that file sizes are much smaller compared to for instance GeoJSON. Compression is of course only applied at chunk level, to retain the property of partial reads.\n\n\n\nDuckDB is an in-memory SQL engine that can run for instance as part of a Python script, or even in a web browser. It is built for speed, and can handle large datasets. It will utilize all available cpu’s, and in our tests could perform analytical queries on RDM data very fast.\nIt support the spatial SQL extension that can also be found in PostGIS, and an H3 extension for working with H3 hexagons. This means that many of the API requests that are normally handled by a web service backed by a SQL database, can just as well be handled by DuckDB.\nThe major advantage of client-side SQL is that you don’t need a server, drastically lowering resource and maintenance cost. Another key advantage, is that users can run any SQL query they like, without needing to ask for a new API endpoint.\n\n\n\nNote that, to validate these technologies, we have effectively performed experiments using WorldCereal reference datasets. These experiments mainly confirmed the statements made above.\n\n\n\n\nBased on these key components, the high-level design is as follows:\nAll data is stored as GeoParquet. Large files are partitioned by H3 index. This allows to very quickly find the datasets that are relevant for a specific AOI. Partitioning avoids overly large files that may still become problematic when being processed in a single call.\nSTAC items can be made to point to the subfiles in a collection. This allows to use either STAC or GeoParquet partitioning to find required files.\nA STAC catalog is built to allow users to find the data they need. Both a static and dynamic catalog are relevant here. Static can be considered a ‘long term’ archive/backup that is always online. Dynamic web service based catalogs helps with discoverability and easy searching.\nNote that it’s even an option to use GeoParquet to store collection metadata as well: this allows to find the right collection very fast.\n\n\nWorldcereal has a need for user-specific collections that are not generally discoverable or accessible.\nThe most basic way in which this design supports such collections, is simply by letting those users manage the STAC and GeoParquet files themselves. There is no web service involved, so there is no need for complex access control and user management.\nThe second option is to make use of STAC collections that are not publicly accessible. This feature is being developed as part of the Terrascope STAC catalog.\nThe security scheme applied here is very important, because when a GeoParquet file is protected by a complex scheme, tools like DuckDB may not be able to support easy reading over http. For instance, a typical OIDC flow is very complex, or else requires the user to get a bearer token, and then to somehow instruct DuckDB to set an HTTP header with that token when trying to access this specific GeoParquet file over http.\nThe proposed solution would be to use url signing: https://github.com/stac-extensions/authentication?tab=readme-ov-file#url-signing This is also used by Microsoft Planetary Computer. It allows the user to generate a specific url that is valid for a limited time, which allows direct access over http.\n\n\n\nTo fully validate the design, proper integration options with other webservices should be explored. In the context of WorldCereal, services based on the openEO standard are most relevant.\nOpenEO already supports two predefined processes that may be relevant: - load_stac to load a vector cube from stac metadata - load_url to load a GeoParquet file from a url\nWhat is missing is a process that allows to filter a vector cube both on spatial region and on properties. Note that the concept of filter pushdown is very important, so the process definition should give backends the freedom to perform this type of optimization.\nWe could also consider to allow SQL style queries on vector cubes, but note that some SQL functionality does overlap with other concepts in openEO.\n\n\n\n\nThese sections validate the design in terms of specific use cases.\n\n\nThe stratification algorithm selects a subset of samples from the full RDM, to be used in model training. It is (normally) a location specific algorithm, as it is generally important to use samples that are properly spatially distributed.\nHence a geospatial index is very important, which is why we propose to add H3 indices to the geoparquet files. By partitioning large (or all) files on this index, we effectively get a data structure that is optimized for spatial queries, without requiring a more complex index.\nThe other problem is how to store the result of the algorithm. There’s a few options here:\n\nadd a column to all GeoParquet files, indicating if a sample is selected.\nCreate a new geoparquet, containing indices of selected samples, allowing to perform a join.\nCreate a new geoparquet, containing the selected subset with full information. (Or at least the information required for further processing.)\n\nIn terms of performance, option number 3 is certainly attractive. It will however come at a storage cost, which is to be evaluated. It is also possible to combine option 3 with either 1 or 2, in the sense that a temporary file can be generated, and stored for the time that it is actively used.\n\n\n\nFor the public data, the extractions are already cached (in another GeoParquet). So openEO can load a vector cube containing extractions for public samples, based on a spatial region.+\nFor user provided data, if the data is public, the caching mechanism should be triggered automatically.\nFor user provided data, if the data is private, the user should invoke a workflow that generates point based extractions. The workflow will require the (signed) url of the private GeoParquet as input, and will generate a Geoparquet with extractions.\nFor the training itself, the user loads 2 vector cubes (public and private extractions), and performs a merge_cubes before invoking the model training operation.",
    "crumbs": [
      "System design",
      "Cloud Native RDM"
    ]
  },
  {
    "objectID": "design/cloudnative_rdm.html#introduction",
    "href": "design/cloudnative_rdm.html#introduction",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "This proposal outlines a potential design for the reference data module, that is based on cloud native file formats and serverless computing.\nThis proposal should be seen as a summary of various ideas and technologies that have been proposed elsewhere (OGC 2017), (Holmes 2023), but applied to ESA WorldCereal context and the problem of managing large datasets of parcel data.",
    "crumbs": [
      "System design",
      "Cloud Native RDM"
    ]
  },
  {
    "objectID": "design/cloudnative_rdm.html#purpose-scope",
    "href": "design/cloudnative_rdm.html#purpose-scope",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "This document explores an RDM design based on some relatively recent, but mature technologies. It is not meant to immediately replace the current webservice based RDM, as there is an important risk that we will still discover issues in the design that are currently unknown, while the RDM webservice in any case already exists.\nBy providing this document, we aim to properly inform about available technologies and how they can be used. We believe this allows to gradually evolve the current setup, or to provide a complementary alternative.",
    "crumbs": [
      "System design",
      "Cloud Native RDM"
    ]
  },
  {
    "objectID": "design/cloudnative_rdm.html#design-drivers",
    "href": "design/cloudnative_rdm.html#design-drivers",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "This section lists a number of key design drivers (or requirements) that have been considered in this design. FAIR & open science principles are a major driver in ESA WorldCereal, but also long term operational costs are an issue.\n\n\nFor a module to be maintainable over a long time after the project, it is key to have a low operational cost. Operational costs are incurred by various factors, but a key point is that any open source component needs an update at some point. This is often triggered by the need to apply security patches, or to be able to implement small enhancements.\nAnother cost factor is of course the number of virtual machines that are needed to run the module.\n\n\n\nFAIR principle A1: (Meta)data are retrievable by their identifier using a standardised communication protocol seems easy to support by using http. The question is however how we can ensure that http links remain valid over time.\nThe ‘cloud native’ solution to this problem is to simply have static versions of data and metadata.\nSo for instance: https://worldcereal-rdm.geo-wiki.org/collections/static/2018beflandersfullpoly110.json could point to a static json file containing STAC collection metadata. It would be relatively easy to maintain such a link in the longer term.\n\n\n\nFair principle R1.3: (Meta)data meet domain-relevant community standards states that domain relevant community standards should be used to describe (meta)data.\nFor the metadata, we propose the use of the STAC standard, complemented with specific extensions where relevant. The use of STAC makes it possible to effectively find reference data that is relevant for a specific area of interest.\nFor the datasets, a real standard does not yet exist, but the GeoParquet format at least allows us to represent the geometry in a standardized manner.\nThe use of the STAC label extension would allow us to describe the legend used by worldcereal in a machine readable way.\n\n\n\nFair principle R1.2: (Meta)data are associated with detailed provenance requires that a dataset or model generated by a system like WorldCereal, can point to the source data that was used to generate it.\nThe RDM enables this by providing the reference data that was used to train models. So the STAC metadata of a model, can point to STAC metadata of reference data collections used to train the model. In STAC, this is typically done using links with a ‘derived-from’ relation type.",
    "crumbs": [
      "System design",
      "Cloud Native RDM"
    ]
  },
  {
    "objectID": "design/cloudnative_rdm.html#proposed-design",
    "href": "design/cloudnative_rdm.html#proposed-design",
    "title": "Design option: serverless RDM",
    "section": "",
    "text": "The proposal can largely be seen as a logical evolution of the current design, combined with principles from cloud native geospatial.\nThe first sentence of (OGC 2017) states: &gt; Cloud-native geospatial offers many benefits to location data users ranging from decreasing the burden on data providers, to drastically lowering the costs of managing that data\nSo the relationship with our own requirement to decrease operational costs should be clear.\nTo make this general idea more specific, we point to a few technological advancements in the past years, that are also explained in the next sections:\n\nGeoParquet as columnar, cloud-native data format.\nDuckDB: an in-memory database that allows to integrate SQL queries on Parquet files in client side scripts or the browser.\n\nThe design is also triggered by a key observation that RDM data is not considered volatile. When new data is added to the RDM, once made public, that version of the data should stay where it is, without changing. Hence we can make technology choices that are optimized for this type of data.\n\n\n\n\nGeoParquet is often referred to as a ‘columnar’ format. This section explains what this stands for and why it is relevant.\nColumnar simply refers to the fact that data is organized by column as opposed to by row. ‘Traditional’ SQL databases like PostGreSQL store data by row, which for instance allows to easily update all values in a row.\nColumnar storage was introduced to address the need for fast analytics queries. For instance, creating a histogram of all croptypes is much faster with columnar, because you only need to load a single column for such a query.\nAnother important property is that GeoParquet is ‘cloud-native’. This usually refers to the simple property of being able to load parts of the data using an HTTP ‘Range’ request. This avoids a full download of the file, when only a small part is needed. This ability can replace a key function that is often performed by web services: reading only a small part of a larger database.\nGeoParquet is also a binary format, with internal compression. This means that file sizes are much smaller compared to for instance GeoJSON. Compression is of course only applied at chunk level, to retain the property of partial reads.\n\n\n\nDuckDB is an in-memory SQL engine that can run for instance as part of a Python script, or even in a web browser. It is built for speed, and can handle large datasets. It will utilize all available cpu’s, and in our tests could perform analytical queries on RDM data very fast.\nIt support the spatial SQL extension that can also be found in PostGIS, and an H3 extension for working with H3 hexagons. This means that many of the API requests that are normally handled by a web service backed by a SQL database, can just as well be handled by DuckDB.\nThe major advantage of client-side SQL is that you don’t need a server, drastically lowering resource and maintenance cost. Another key advantage, is that users can run any SQL query they like, without needing to ask for a new API endpoint.\n\n\n\nNote that, to validate these technologies, we have effectively performed experiments using WorldCereal reference datasets. These experiments mainly confirmed the statements made above.\n\n\n\n\nBased on these key components, the high-level design is as follows:\nAll data is stored as GeoParquet. Large files are partitioned by H3 index. This allows to very quickly find the datasets that are relevant for a specific AOI. Partitioning avoids overly large files that may still become problematic when being processed in a single call.\nSTAC items can be made to point to the subfiles in a collection. This allows to use either STAC or GeoParquet partitioning to find required files.\nA STAC catalog is built to allow users to find the data they need. Both a static and dynamic catalog are relevant here. Static can be considered a ‘long term’ archive/backup that is always online. Dynamic web service based catalogs helps with discoverability and easy searching.\nNote that it’s even an option to use GeoParquet to store collection metadata as well: this allows to find the right collection very fast.\n\n\nWorldcereal has a need for user-specific collections that are not generally discoverable or accessible.\nThe most basic way in which this design supports such collections, is simply by letting those users manage the STAC and GeoParquet files themselves. There is no web service involved, so there is no need for complex access control and user management.\nThe second option is to make use of STAC collections that are not publicly accessible. This feature is being developed as part of the Terrascope STAC catalog.\nThe security scheme applied here is very important, because when a GeoParquet file is protected by a complex scheme, tools like DuckDB may not be able to support easy reading over http. For instance, a typical OIDC flow is very complex, or else requires the user to get a bearer token, and then to somehow instruct DuckDB to set an HTTP header with that token when trying to access this specific GeoParquet file over http.\nThe proposed solution would be to use url signing: https://github.com/stac-extensions/authentication?tab=readme-ov-file#url-signing This is also used by Microsoft Planetary Computer. It allows the user to generate a specific url that is valid for a limited time, which allows direct access over http.\n\n\n\nTo fully validate the design, proper integration options with other webservices should be explored. In the context of WorldCereal, services based on the openEO standard are most relevant.\nOpenEO already supports two predefined processes that may be relevant: - load_stac to load a vector cube from stac metadata - load_url to load a GeoParquet file from a url\nWhat is missing is a process that allows to filter a vector cube both on spatial region and on properties. Note that the concept of filter pushdown is very important, so the process definition should give backends the freedom to perform this type of optimization.\nWe could also consider to allow SQL style queries on vector cubes, but note that some SQL functionality does overlap with other concepts in openEO.\n\n\n\n\nThese sections validate the design in terms of specific use cases.\n\n\nThe stratification algorithm selects a subset of samples from the full RDM, to be used in model training. It is (normally) a location specific algorithm, as it is generally important to use samples that are properly spatially distributed.\nHence a geospatial index is very important, which is why we propose to add H3 indices to the geoparquet files. By partitioning large (or all) files on this index, we effectively get a data structure that is optimized for spatial queries, without requiring a more complex index.\nThe other problem is how to store the result of the algorithm. There’s a few options here:\n\nadd a column to all GeoParquet files, indicating if a sample is selected.\nCreate a new geoparquet, containing indices of selected samples, allowing to perform a join.\nCreate a new geoparquet, containing the selected subset with full information. (Or at least the information required for further processing.)\n\nIn terms of performance, option number 3 is certainly attractive. It will however come at a storage cost, which is to be evaluated. It is also possible to combine option 3 with either 1 or 2, in the sense that a temporary file can be generated, and stored for the time that it is actively used.\n\n\n\nFor the public data, the extractions are already cached (in another GeoParquet). So openEO can load a vector cube containing extractions for public samples, based on a spatial region.+\nFor user provided data, if the data is public, the caching mechanism should be triggered automatically.\nFor user provided data, if the data is private, the user should invoke a workflow that generates point based extractions. The workflow will require the (signed) url of the private GeoParquet as input, and will generate a Geoparquet with extractions.\nFor the training itself, the user loads 2 vector cubes (public and private extractions), and performs a merge_cubes before invoking the model training operation.",
    "crumbs": [
      "System design",
      "Cloud Native RDM"
    ]
  },
  {
    "objectID": "design/processing_system.html",
    "href": "design/processing_system.html",
    "title": "openEO based processing",
    "section": "",
    "text": "The architecture of WorldCereal phase II is based upon the guidelines provided by the ESA APEx project. The key goal is to build a system and processing workflows that can be used beyond the project lifetime.\nIn accordance with the APEx framework, a few project specific choices have been made:\n\nopenEO is selected as the standard in which processing workflows are described\nCopernicus Dataspace Ecosystem openEO federation is selected as the target platform\n\n\n\nWorldCereal selects openEO as processing standard, because it allows to express both the training and inference workflows in a high level manner. While in phase I, the consortium had to invest substantially in writing data access code, this is now handled by openEO.\nThe use of openEO aids in generating results that follow FAIR data and open science principles. This is crucial to provide transparency and reproducibility of WorldCereal results. It will also make it very feasible for anyone to inspect workflows in a visual manner.\n\n\n\nThe scalable processing needed to generate a global map is offered by the CDSE openEO federation. This includes the software to manage cloud resources, the software to parallelize workflows, disaster recovery across 2 datacenters, and the operational burden of monitoring the system.\nThe CDSE open source deployment is described in more detail in the CDSE openEO documentation. The most important element is that CDSE components are required to maintain an uptime of 99.5% on a monthly basis. Lower uptimes are possible, but result in penalties for the platform operators. The contract also has a long lifetime, ensuring that WorldCereal workflows can be executed beyond the project lifetime.\nFinally, it is important to note that CDSE is the only EU platform that offers access to the full archives of Sentinel-2 L2A and Sentinel-1 GRD data. Hence, for global map production there are currently no alternatives, except for non-EU commercial cloud providers.",
    "crumbs": [
      "System design",
      "Processing System"
    ]
  },
  {
    "objectID": "design/processing_system.html#openeo-as-api",
    "href": "design/processing_system.html#openeo-as-api",
    "title": "openEO based processing",
    "section": "",
    "text": "WorldCereal selects openEO as processing standard, because it allows to express both the training and inference workflows in a high level manner. While in phase I, the consortium had to invest substantially in writing data access code, this is now handled by openEO.\nThe use of openEO aids in generating results that follow FAIR data and open science principles. This is crucial to provide transparency and reproducibility of WorldCereal results. It will also make it very feasible for anyone to inspect workflows in a visual manner.",
    "crumbs": [
      "System design",
      "Processing System"
    ]
  },
  {
    "objectID": "design/processing_system.html#cdse-as-processing-platform",
    "href": "design/processing_system.html#cdse-as-processing-platform",
    "title": "openEO based processing",
    "section": "",
    "text": "The scalable processing needed to generate a global map is offered by the CDSE openEO federation. This includes the software to manage cloud resources, the software to parallelize workflows, disaster recovery across 2 datacenters, and the operational burden of monitoring the system.\nThe CDSE open source deployment is described in more detail in the CDSE openEO documentation. The most important element is that CDSE components are required to maintain an uptime of 99.5% on a monthly basis. Lower uptimes are possible, but result in penalties for the platform operators. The contract also has a long lifetime, ensuring that WorldCereal workflows can be executed beyond the project lifetime.\nFinally, it is important to note that CDSE is the only EU platform that offers access to the full archives of Sentinel-2 L2A and Sentinel-1 GRD data. Hence, for global map production there are currently no alternatives, except for non-EU commercial cloud providers.",
    "crumbs": [
      "System design",
      "Processing System"
    ]
  },
  {
    "objectID": "design/processing_system.html#processing-system-design",
    "href": "design/processing_system.html#processing-system-design",
    "title": "openEO based processing",
    "section": "Processing system design",
    "text": "Processing system design\nThe design of the processing system is focused on 3 main use cases:\n\nGenerating maps based on existing models\nTraining new models based on a user-defined selection of reference data\nDisseminating products\n\nThese processes, and the links to various components are shown in the diagram below. Key operational components are supported by the CDSE and APEx projects, to ensure that the system remains operational beyond project lifetime.\n\n\n\nProcessing system\n\n\nThe WorldCereal Toolbox is an open source Python library and set of Jupyter notebooks, that can be used to train custom models. It offers a user-friendly API and easy example code, to make the process as easy as possible.\nThis toolbox also contains other WorldCereal specific source code that was used to develop the system, or to generate openEO UDPs. It is based on GFMap, a general purpose, openEO-based, library for mapping applications.\nThe WorldCereal openEO UDPs are user defined processes that encapsulate specific WorldCereal workflows. These workflows can be executed from generic tools such as the openEO web editor, and are hosted in APEx.\nThe APEx Algorithm hosting service maintains a catalog of openEO UDPs, ensuring that they remain discoverable beyond the project lifetime, and allowing initiatives such as the ESA Stakeholder engagement facility to promote the use of WorldCereal services. The hosting service also runs regular benchmarks and tests, to be able to flag if an algorithm is no longer functional.\nThe APEx Upscaling service allows to produce results over a larger area, based on openEO UDPs. This allows WorldCereal users to generate larger maps.\nThe STAC catalog is used for dissemination and long term archiving of maps generated by the project. An openEO backend can export results directly to STAC, or this can be performed by the WorldCereal Toolbox.",
    "crumbs": [
      "System design",
      "Processing System"
    ]
  },
  {
    "objectID": "design/reference_data_module.html",
    "href": "design/reference_data_module.html",
    "title": "Reference Data Management",
    "section": "",
    "text": "While the RDM is built on top of a PostGIS database, we more and more start to use H3 for fast geospatial lookup. H3 is a discrete global grid system, with some nice properties for cases like ours.\nOne example use case is creating a heatmap showing distribution of samples for specific crop types over the world.\nH3 indexes can also be computed on the fly from the geometry, but a lot of operations can be made faster if they are performed on 64bit integers rather than geometries.\n\n\nThis can be achieved in different technology stacks. For instance, in (postgres)[https://github.com/zachasme/h3-pg]:\nSELECT h3_lat_lng_to_cell(POINT('37.3615593,-122.0553238'), 5)\nor in (Python)[https://uber.github.io/h3-py/intro.html#usage]:\nh3.latlng_to_cell(lat, lng, resolution)\nor other options: https://h3geo.org/docs/community/bindings",
    "crumbs": [
      "System design",
      "Reference Data Module"
    ]
  },
  {
    "objectID": "design/reference_data_module.html#reading-test-from-parquet",
    "href": "design/reference_data_module.html#reading-test-from-parquet",
    "title": "Reference Data Management",
    "section": "Reading test from Parquet",
    "text": "Reading test from Parquet\n\n%%time\nimport geopandas as gpd\nimport fsspec\npq_path = \"https://ewocstorage.blob.core.windows.net/collections/2021_PT_EUROCROP_POLY_110.parquet\"\nwith fsspec.open(pq_path) as file:\n    df = gpd.read_parquet(file,columns=[\"geometry\",\"CT\"])\ndf\n\nCPU times: user 909 ms, sys: 305 ms, total: 1.21 s\nWall time: 17.3 s\n\n\n\n\n\n\n\n\n\n\ngeometry\nCT\n\n\n\n\n0\nMULTIPOLYGON (((-8.54796 40.56554, -8.54942 40...\n1200\n\n\n1\nMULTIPOLYGON (((-8.52352 40.55686, -8.52352 40...\n1700\n\n\n2\nMULTIPOLYGON (((-8.52456 40.55538, -8.52454 40...\n3300\n\n\n3\nMULTIPOLYGON (((-8.52835 40.56835, -8.52837 40...\n2000\n\n\n4\nMULTIPOLYGON (((-8.52781 40.57128, -8.52814 40...\n0\n\n\n...\n...\n...\n\n\n99995\nMULTIPOLYGON (((-6.46866 41.43669, -6.46832 41...\n0\n\n\n99996\nMULTIPOLYGON (((-6.46797 41.43497, -6.46778 41...\n0\n\n\n99997\nMULTIPOLYGON (((-7.45134 41.73074, -7.45134 41...\n1200\n\n\n99998\nMULTIPOLYGON (((-6.47482 41.44217, -6.47480 41...\n0\n\n\n99999\nMULTIPOLYGON (((-6.47503 41.44182, -6.47504 41...\n9520\n\n\n\n\n100000 rows × 2 columns\n\n\n\n\n\ndf.groupby(['CT']).count()\n\n\n\n\n\n\n\n\n\ngeometry\n\n\nCT\n\n\n\n\n\n0\n37673\n\n\n1100\n191\n\n\n1200\n4475\n\n\n1300\n271\n\n\n1500\n69\n\n\n...\n...\n\n\n9300\n10\n\n\n9320\n3\n\n\n9500\n8\n\n\n9520\n971\n\n\n9920\n4\n\n\n\n\n61 rows × 1 columns\n\n\n\n\n\n%%time\nimport duckdb\ndb = duckdb.connect()\ndb.execute('select count(*) from read_parquet(\"https://ewocstorage.blob.core.windows.net/collections/2021_PT_EUROCROP_POLY_110.parquet\")').fetchall()\n\nCPU times: user 255 ms, sys: 4.75 ms, total: 260 ms\nWall time: 294 ms\n\n\n[(100000,)]\n\n\n\n%%time \ndb.query('select CT,count(*) from read_parquet(\"https://ewocstorage.blob.core.windows.net/collections/2021_PT_EUROCROP_POLY_110.parquet\") GROUP BY CT').to_df()\n\nCPU times: user 48.1 ms, sys: 17.3 ms, total: 65.3 ms\nWall time: 597 ms\n\n\n\n\n\n\n\n\n\n\nCT\ncount_star()\n\n\n\n\n0\n9100\n21039\n\n\n1\n2000\n1976\n\n\n2\n3530\n153\n\n\n3\n7100\n188\n\n\n4\n4380\n11\n\n\n...\n...\n...\n\n\n56\n1200\n4475\n\n\n57\n9520\n971\n\n\n58\n7300\n16\n\n\n59\n2190\n1\n\n\n60\n3490\n4\n\n\n\n\n61 rows × 2 columns\n\n\n\n\n\ndb.execute('INSTALL spatial;LOAD spatial;')\n\n&lt;duckdb.duckdb.DuckDBPyConnection at 0x70e94825ff30&gt;\n\n\n\n%%time\ndb.query('select ST_centroid(ST_GeomFromWKB(geometry)) from read_parquet(\"https://ewocstorage.blob.core.windows.net/collections/2021_PT_EUROCROP_POLY_110.parquet\") USING SAMPLE 100 ROWS ').to_df()\n\n\n\n\nCPU times: user 4.88 s, sys: 92.6 ms, total: 4.97 s\nWall time: 11.8 s\n\n\n\n\n\n\n\n\n\n\nst_centroid(st_geomfromwkb(geometry))\n\n\n\n\n0\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n1\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n2\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n3\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n4\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n...\n...\n\n\n95\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n96\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n97\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n98\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n99\n[0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,...\n\n\n\n\n100 rows × 1 columns\n\n\n\n\n\nGeoParquet vs OGC Features\nAs shown above, both DuckDB and GeoPandas can efficiently handle Parquet files of 100k items stored on https. With parquet as interface, data scientists can write complex queries in a language they know (Pandas, SQL, …).\nWhen looking at OGC features, it seems there are hardly any libraries available. Some basic support in GDAL seems to be the best option to connect with it. To benefit from server side processing power, the most comfortable option seems to write CQL filters. OGC Features does not support aggregation, so a ‘group by’ operation is not supported.\nNote: GeoParquet is on track to be adopted as an OGC standard, hence satisfies standardization requirements.",
    "crumbs": [
      "System design",
      "Reference Data Module"
    ]
  },
  {
    "objectID": "design/inference.html",
    "href": "design/inference.html",
    "title": "Crop type map production",
    "section": "",
    "text": "Generating a WorldCereal product based on an existing model basically involves running the corresponding UDP on the CDSE openEO federation. There are multiple ways to do this, depending on the user’s needs and preferences, so consulting the online documentation is the best way forward. We will however elaborate a few approaches here, to illustrate the possibilities.\nNote that all possibilities require that the user has either requested access to the service via ESA network of resources, or uses public service credits available on CDSE free of charge.\nThe WorldCereal VDM will allow to trigger processing via a web UI. This is a purpose built interface for WorldCereal, aiding the user in setting the correct parameters. Similarly, the ESA APEx portal is also expected to offer a similar capability, but based on a generic user interface. The generated results can be downloaded as Geotiff, for visualization in QGis.\nWhen the user wants to generate maps for larger areas, there is the option to use the APEx ‘upscaling service’, which is built for this purpose, or the user can resort to using Python based tools such as GFMap to run a script that generates the map. In both cases, the actual execution again happens on CDSE openEO.\nFinally, a Python notebook part of WorldCereal toolbox will show how to generate results.",
    "crumbs": [
      "System design",
      "Inference"
    ]
  },
  {
    "objectID": "design/inference.html#general-flow",
    "href": "design/inference.html#general-flow",
    "title": "Crop type map production",
    "section": "",
    "text": "Generating a WorldCereal product based on an existing model basically involves running the corresponding UDP on the CDSE openEO federation. There are multiple ways to do this, depending on the user’s needs and preferences, so consulting the online documentation is the best way forward. We will however elaborate a few approaches here, to illustrate the possibilities.\nNote that all possibilities require that the user has either requested access to the service via ESA network of resources, or uses public service credits available on CDSE free of charge.\nThe WorldCereal VDM will allow to trigger processing via a web UI. This is a purpose built interface for WorldCereal, aiding the user in setting the correct parameters. Similarly, the ESA APEx portal is also expected to offer a similar capability, but based on a generic user interface. The generated results can be downloaded as Geotiff, for visualization in QGis.\nWhen the user wants to generate maps for larger areas, there is the option to use the APEx ‘upscaling service’, which is built for this purpose, or the user can resort to using Python based tools such as GFMap to run a script that generates the map. In both cases, the actual execution again happens on CDSE openEO.\nFinally, a Python notebook part of WorldCereal toolbox will show how to generate results.",
    "crumbs": [
      "System design",
      "Inference"
    ]
  },
  {
    "objectID": "design/inference.html#on-premise-execution",
    "href": "design/inference.html#on-premise-execution",
    "title": "Crop type map production",
    "section": "On premise execution",
    "text": "On premise execution\nFor users that wish to generate results on their own infrastructure, a local openEO deployment will be needed.\nTo achieve this, the CDSE openEO backend software is also made available in a docker image, allowing to start an openEO application on a single node. This assumes that nodes are used with sufficient CPU and memory resources. Based on current experience, a node with 32 cores and 256GB of memory is sufficient to run the workflow, this should match with commonly available server class hardware. The development of this docker image and supporting documentation is expected to be performed in the ESA EOEPCA project.\nFor data access, the easiest option is to provide a configuration that enables remote access to the Copernicus Dataspace Ecosystem catalog and object storage. Alternatively, users can set up local STAC catalogs, that mirror relevant parts of the Copernicus Dataspace Ecosystem catalog. The local openEO deployment can then reference the local catalog and datasets. For mirroring of data, we refer to the EO-DAG tool as one option which is recommended by the ESA EOEPCA project. Note however that efficient and production-ready data mirroring is beyond the scope of WorldCereal, so is the full responsibility of the user that wishes to operate processing locally.",
    "crumbs": [
      "System design",
      "Inference"
    ]
  },
  {
    "objectID": "design/inference.html#production-workflow",
    "href": "design/inference.html#production-workflow",
    "title": "Crop type map production",
    "section": "Production workflow",
    "text": "Production workflow\nWorldCereal products are generated by openEO workflows. The workflow requires a trained CatBoost model, which is a parameter because users may want to use their own models.\nThe pseudo code below outlines the general steps of the inference pipeline.\n\nimport  openeo\nfrom    openeo.rest.mlmodel import MlModel\nfrom    openeo.processes import ProcessBuilder\n\nconnection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\n1l2A = connection.load_collection(\"SENTINEL2_L2A\").aggregate_temporal_period(period=\"month\",reducer=\"mean\")\nsentinel1 = connection.load_collection(\"SENTINEL1_GRD\")\nbs = sentinel1.sar_backscatter(coefficient=\"sigma0-ellipsoid\").resample_spatial(resolution=20).aggregate_temporal_period(period=\"month\",reducer=\"mean\")\n\n2\n\nfrom    openeo import UDF\nfeature_udf=UDF(code=\"\",runtime=\"Python\") #load UDF to compute presto features based on monthly timeseries\nfeatures_cube = l2A.merge_cubes(bs).apply_dimension(dimension='t',process=feature_udf,target_dimension='bands')\n\n\nmodel = MlModel.load_ml_model(connection=connection, id=\"http://myhost.com/my_catboost_stac_metadata.json\")\n\ncatboost_classifier = lambda data, context: ProcessBuilder.process(\"predict_catboost\",data=data, model=context)\nworldcereal_product = features_cube.reduce_dimension(dimension=\"bands\", reducer=catboost_classifier, context=model)\n\nworldcereal_product\n\n\n1\n\ninstead of aggregate_temporal, we’ll do more advanced compositing, such as max-NDVI\n\n2\n\nwe’ll need to add agera5 and dem bands\n\n\n\n\n\n\n\n    \n    \n        \n    \n    \n\n\nFigure 1: The WorldCereal inference pipeline",
    "crumbs": [
      "System design",
      "Inference"
    ]
  },
  {
    "objectID": "design/inference.html#exporting-results-to-workspace",
    "href": "design/inference.html#exporting-results-to-workspace",
    "title": "Crop type map production",
    "section": "Exporting results to workspace",
    "text": "Exporting results to workspace\nThe openEO backend can store generated products directly in a custom object storage location. This is an optional step, but convenient when trying to avoid copying around files.\nNext to storing the file, it is also important to update and store the STAC metadata.\n\nstac_metadata = worldcereal_product.save_result(format=\"GTiff\")\nstac_metadata = connection.datacube_from_process(\"stac_update\",data = stac_metadata) #todo: add custom metadata\n\nconnection.datacube_from_process(\"export_workspace\",data = stac_metadata, workspace = \"my_workspace\", merge=\"pointer_to_worldcereal_collection\")\n\n\n\n\n    \n    \n        \n    \n    \n\n\nFigure 2: Workflow steps to export results to object storage",
    "crumbs": [
      "System design",
      "Inference"
    ]
  }
]