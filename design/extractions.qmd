## Extractions cache

### First level cache

The first level cache is a collection of netCDF raster files, all with a fixed size of e.g. 64x64 pixels.

### Extraction workflow steps

1. Get id of extraction to run
2. For extraction id get point locations from RDM
3. Use UDF to convert points into 64x64 patches



### First level cache requirements

1. netCDF assets need to link back to the sample from which they were generated.
2. a 'Ground truth' asset contains the raster with ground truth info, meaning the croptype code.
3. Sentinel-2 asset at 10m resolution.
4. Sentinel-1 asset at 20m resolution.
5. AgERA5 asset

STAC extensions:
- projection (proj) provides detailed info on raster size and projection system


#### Collection metadata
```json
{{< include stac_examples/l1_cache_collection.json >}}
```


### Cache updates

The RDM needs to be queried for new collections on a regular basis, to discover new collections.

#### Option 1: Nifi

http fetch collections
-> DetectDuplicate/DeduplicateRecord for fast duplicate dropping
-> LookupRecord to check if we already know about the collection

Query SQL

New collections become flow files

Per new collection, do job splitting.

Continuously run job job manager on job splits.

Monitoring: NiFi processors to send mail

#### Option 2: Kubernetes cron job

Kubernetes can schedule cron job, allowing to run Python script on a daily basis, detecting new collections.

Monitoring: alertmanager




Feature required: job manager write to Parquet on S3?
Feature GFMap: write to workspace
Or User Workspace with http access?
Or as 'upcscaling service' pod in k8s?

Dashboard: