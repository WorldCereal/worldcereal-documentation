# Model training EO data management

To effectively train models, we need very fast access to the input data at the locations where ground truth information
is available. The WorldCereal classifier is primarily looking at the time-series for multiple bands per pixel.
The native Sentinel earth observation data archives are not stored in a format that enables fast time series access. For
instance, a Sentinel-2 product uses internal chunks of 1000x1000 pixels, while for training we need only a few 64x64 pixel
chunks. Hence, a lot of unnecessary data is read from relatively slow storage when constructing such a timeseries.

A more favourable model is thus to generate files that contain the full timeseries and all bands for a given sensor. A single
read operation can then load a full year worth of EO data for a location where reference data is available.

What the training step requires is analysis-ready data, which in the case of WorldCereal currently means single-pixel, cloud-free monthly composites.
As there currently does not yet exist a consensus on how to best generate this analysis-ready data. The design choice was
made to store the data in 2 formats:

1. Raster files containing 64x64 pixel chunks of raw EO data observations, minimal cloud-screening.
2. Parquet files containing analysis-ready pixel timeseries, with all input bands and the ground-truth label.

Having these two formats enables experimentation at both preprocessing level, and the level of model tuning on ARD data.
If at some point the spatial context of a pixel is taken into account, this information is also available.

In the sections below, this general design is detailed further. We generally refer to the locally stored EO data as 'the extractions',
because they are extracts from the main EO archive.

## Extractions cache

### First level cache

The first level cache is a collection of netCDF raster files, all with a fixed size of e.g. 64x64 pixels.

### Extraction workflow steps

1. Get id of extraction to run
2. For extraction id get point locations from RDM
3. Use UDF to convert points into 64x64 patches



### First level cache requirements

1. netCDF assets need to link back to the sample from which they were generated.
2. a 'Ground truth' asset contains the raster with ground truth info, meaning the croptype code.
3. Sentinel-2 asset at 10m resolution.
4. Sentinel-1 asset at 20m resolution.
5. AgERA5 asset

STAC extensions:
- projection (proj) provides detailed info on raster size and projection system


#### Collection metadata
```json
{{< include stac_examples/l1_cache_collection.json >}}
```


### Cache updates

The RDM needs to be queried for new collections on a regular basis, to discover new collections. Whenever a new collection
is available in the RDM, we want the extraction workflow to automatically update the cache, allowing users to train models
efficiently on all data available in the RDM.


#### Option 1: Nifi

http fetch collections
-> DetectDuplicate/DeduplicateRecord for fast duplicate dropping
-> LookupRecord to check if we already know about the collection

Query SQL

New collections become flow files

Per new collection, do job splitting.

Continuously run job job manager on job splits.

Monitoring: NiFi processors to send mail

#### Option 2: Kubernetes cron job

Kubernetes can schedule cron job, allowing to run Python script on a daily basis, detecting new collections.

Monitoring: alertmanager




Feature required: job manager write to Parquet on S3?
Feature GFMap: write to workspace
Or User Workspace with http access?
Or as 'upcscaling service' pod in k8s?

Dashboard: